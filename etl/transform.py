# transform.py

import logging
import pandas as pd
from pyspark.sql.functions import (
    col,
    when,
    lit,
    sum as spark_sum,
    round,
    regexp_replace,
    regexp_extract,
    expr,
    trim,
    upper,
    create_map,
)
from pyspark.sql.types import IntegerType, DoubleType, DateType, StringType, StructType, StructField
from pyspark.sql.window import Window
from pyspark.sql import functions as F, types as T, SparkSession
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler
from itertools import chain

# Configuration du logger
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DataTransformer:
    """
    Classe permettant de transformer les donn√©es extraites avant leur chargement.
    """

    def __init__(self):
        logger.info("üöÄ Initialisation du DataTransformer")

    def transform_environmental_data(self, df_env):
        """
        Transforme les donn√©es environnementales :
        - S√©lectionne uniquement les colonnes n√©cessaires
        - Remplace les valeurs vides par 0.0 pour les valeurs manquantes d'√©olien et solaire
        - Ajoute les donn√©es de l'ann√©e 2000 pour chaque r√©gion avec des valeurs √† 0.0
        - Regroupe les donn√©es par Code_INSEE_r√©gion et Ann√©e (somme en cas de doublons)
        - Trie les r√©sultats par r√©gion et ann√©e
        """

        if df_env is None:
            logger.error("‚ùå Le DataFrame environnemental est vide ou invalide.")
            return None

        logger.info("üöÄ Transformation des donn√©es environnementales en cours...")

        # S√©lection des colonnes n√©cessaires et cast des valeurs
        df_transformed = df_env.select(
            col("Ann√©e").cast("int"),
            col("Code_INSEE_R√©gion"),
            col("Parc_install√©_√©olien_MW").cast("double"),
            col("Parc_install√©_solaire_MW").cast("double"),
        )

        # Remplacement des valeurs nulles par 0.0
        df_transformed = df_transformed.fillna(
            {"Parc_install√©_√©olien_MW": 0.0, "Parc_install√©_solaire_MW": 0.0}
        )

        # Regroupement par r√©gion et ann√©e pour sommer les valeurs en cas de doublons
        df_grouped = df_transformed.groupBy("Code_INSEE_R√©gion", "Ann√©e").agg(
            spark_sum("Parc_install√©_√©olien_MW").alias("Parc_install√©_√©olien_MW"),
            spark_sum("Parc_install√©_solaire_MW").alias("Parc_install√©_solaire_MW"),
        )

        # R√©cup√©ration des r√©gions uniques pr√©sentes dans les donn√©es
        regions = df_grouped.select("Code_INSEE_R√©gion").distinct()

        # Cr√©ation d'un DataFrame contenant l'ann√©e 2000 pour chaque r√©gion avec valeurs √† 0.0
        df_year_2000 = (
            regions.withColumn("Ann√©e", lit(2000))
            .withColumn("Parc_install√©_√©olien_MW", lit(0.0))
            .withColumn("Parc_install√©_solaire_MW", lit(0.0))
        )

        # Ajout des donn√©es de l'ann√©e 2000 au DataFrame principal
        df_final = df_grouped.union(df_year_2000)

        # Tri des donn√©es par r√©gion et ann√©e
        df_final = df_final.orderBy("Code_INSEE_R√©gion", "Ann√©e")

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "‚úÖ Transformation termin√©e ! Aper√ßu des donn√©es transform√©es :",
            df_final,
            15,
        )

    def transform_pib_outre_mer(self, df_pib, region_codes):
        """
        Transforme les donn√©es PIB outre-mer :
        - Suppression des lignes inutiles
        - Ajout du code r√©gion INSEE √† partir du nom du fichier
        - Tri par R√©gion puis Ann√©e
        """

        if df_pib is None:
            logger.error("‚ùå Le DataFrame PIB est vide ou invalide.")
            return None

        logger.info("üöÄ Transformation des donn√©es PIB outre-mer en cours...")

        # Nettoyage des donn√©es
        df_cleaned = df_pib.filter(
            (~col("Ann√©e").isin(["idBank", "Derni√®re mise √† jour", "P√©riode"]))
            & (col("Ann√©e").rlike("^[0-9]{4}$"))
        ).select(
            col("Ann√©e").cast("int"),
            col("PIB_en_euros_par_habitant").cast("int"),
            col("source_file"),
        )

        # Ajout du code r√©gion INSEE depuis le dictionnaire region_codes
        condition = None
        for file_path, code_region in region_codes.items():
            if condition is None:
                condition = when(col("source_file") == file_path, lit(code_region))
            else:
                condition = condition.when(
                    col("source_file") == file_path, lit(code_region)
                )

        df_final = df_cleaned.withColumn("Code_INSEE_R√©gion", lit(None))
        for file_path, code_region in region_codes.items():
            df_final = df_final.withColumn(
                "Code_INSEE_R√©gion",
                when(col("source_file") == file_path, lit(code_region)).otherwise(
                    col("Code_INSEE_R√©gion")
                ),
            )

        df_final = df_final.drop("source_file")

        # Tri final
        df_final = df_final.orderBy(["Code_INSEE_R√©gion", "Ann√©e"])

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "‚úÖ Transformation PIB termin√©e ! Aper√ßu des donn√©es transform√©es :",
            df_final,
            10,
        )

    def fill_missing_pib_mayotte(self, df_pib):
        """
        Remplit les valeurs manquantes du PIB de Mayotte par r√©gression lin√©aire.
        """

        logger.info("üöÄ Remplissage des valeurs manquantes PIB Mayotte en cours...")

        df_mayotte = df_pib.filter(col("Code_INSEE_R√©gion") == "06")

        known_data = df_mayotte.filter(col("PIB_en_euros_par_habitant").isNotNull())
        unknown_data = df_mayotte.filter(col("PIB_en_euros_par_habitant").isNull())

        assembler = VectorAssembler(inputCols=["Ann√©e"], outputCol="features")
        train_data = assembler.transform(known_data).select(
            "features", "PIB_en_euros_par_habitant"
        )

        # Mod√®le de r√©gression lin√©aire
        lr = LinearRegression(
            featuresCol="features", labelCol="PIB_en_euros_par_habitant"
        )
        model = lr.fit(train_data)

        # Pr√©dictions sur les donn√©es manquantes
        pred_df = assembler.transform(unknown_data)
        pred_result = model.transform(pred_df).select(
            "Ann√©e",
            col("prediction").cast("int").alias("PIB_en_euros_par_habitant"),
            "Code_INSEE_R√©gion",
        )

        # Combine les donn√©es connues et pr√©dites
        df_mayotte_completed = known_data.select(
            "Ann√©e", "PIB_en_euros_par_habitant", "Code_INSEE_R√©gion"
        ).union(pred_result)

        # Autres r√©gions sans modifications
        df_other_regions = df_pib.filter(col("Code_INSEE_R√©gion") != "06")

        # Union finale
        df_final = df_other_regions.union(df_mayotte_completed).orderBy(
            ["Code_INSEE_R√©gion", "Ann√©e"]
        )

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "‚úÖ Remplissage PIB Mayotte termin√© :", df_final, 10
        )

    def combine_all_pib_data(self, df_pib_outremer, df_pib_xlsx, df_pib_2022):
        """
        Combine les donn√©es PIB des diff√©rentes sources en un seul DataFrame.
        """

        logger.info("üöÄ Fusion des donn√©es PIB (Outre-mer, Excel, 2022)...")

        # Harmoniser les colonnes
        df_pib_xlsx = df_pib_xlsx.select(
            "Ann√©e", "PIB_en_euros_par_habitant", "Code_INSEE_R√©gion"
        )
        df_pib_2022 = df_pib_2022.select(
            "Ann√©e", "PIB_en_euros_par_habitant", "Code_INSEE_R√©gion"
        )
        df_pib_outremer = df_pib_outremer.select(
            "Ann√©e", "PIB_en_euros_par_habitant", "Code_INSEE_R√©gion"
        )

        # Liste des r√©gions pr√©sentes en 2022
        regions_2022 = [
            row["Code_INSEE_R√©gion"]
            for row in df_pib_2022.select("Code_INSEE_R√©gion").distinct().collect()
        ]

        # Identifier les r√©gions absentes en 2022
        missing_regions = (
            df_pib_xlsx.select("Code_INSEE_R√©gion")
            .distinct()
            .filter(~col("Code_INSEE_R√©gion").isin(regions_2022))
        )

        # Ajouter des lignes vides pour les r√©gions absentes en 2022
        if missing_regions.count() > 0:
            df_missing_2022 = missing_regions.withColumn("Ann√©e", lit(2022)).withColumn(
                "PIB_en_euros_par_habitant", lit(None).cast("int")
            )
            df_pib_2022 = df_pib_2022.union(df_missing_2022)

        # Fusion des donn√©es
        df_final = df_pib_outremer.union(df_pib_xlsx).union(df_pib_2022)

        # **Filtrer les lignes invalides** (Code r√©gion doit √™tre num√©rique et PIB non NULL)
        df_final = df_final.filter(
            (col("Code_INSEE_R√©gion").rlike("^[0-9]+$"))
            & (col("PIB_en_euros_par_habitant").isNotNull())
        )

        # Filtrer et trier
        df_final = df_final.filter((col("Ann√©e") >= 2000) & (col("Ann√©e") <= 2022))
        df_final = df_final.orderBy(["Code_INSEE_R√©gion", "Ann√©e"])

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "‚úÖ Fusion des donn√©es PIB r√©ussie :", df_final, 10
        )

    def transform_inflation_data(self, df_inflation):
        """
        Transforme les donn√©es d'inflation en filtrant les ann√©es et en les triant.

        :param df_inflation: DataFrame PySpark contenant les donn√©es brutes d'inflation.
        :return: DataFrame PySpark nettoy√© et tri√©.
        """
        if df_inflation is None:
            logger.error("‚ùå Le DataFrame inflation est vide ou invalide.")
            return None

        logger.info("üöÄ Transformation des donn√©es d'inflation en cours...")

        # Filtrer et trier les donn√©es
        df_transformed = df_inflation.orderBy("Ann√©e")

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "‚úÖ Transformation des donn√©es d'inflation r√©ussie :",
            df_transformed,
            10,
        )

    def combine_pib_and_inflation(self, df_pib, df_inflation):
        """
        Combine les donn√©es PIB et Inflation, et calcule le ratio PIB_par_inflation avec arrondi √† 2 d√©cimales.

        :param df_pib: DataFrame PySpark contenant le PIB par r√©gion.
        :param df_inflation: DataFrame PySpark contenant l'inflation nationale.
        :return: DataFrame PySpark combin√© avec le calcul du PIB ajust√© par l'inflation.
        """
        if df_pib is None or df_inflation is None:
            logger.error("‚ùå L'un des DataFrames est vide. Impossible de les combiner.")
            return None

        logger.info("üöÄ Fusion des donn√©es PIB et Inflation...")

        # Joindre PIB et Inflation sur la colonne Ann√©e
        df_combined = df_pib.join(df_inflation, "Ann√©e", "left")

        # Utiliser le bon nom de colonne pour l'inflation et arrondir √† 2 d√©cimales
        df_combined = df_combined.withColumn(
            "√âvolution_des_prix_√†_la_consommation",
            round(col("√âvolution_des_prix_√†_la_consommation"), 2),
        )

        df_combined = df_combined.withColumn(
            "PIB_par_inflation",
            round(
                col("PIB_en_euros_par_habitant")
                / (1 + col("√âvolution_des_prix_√†_la_consommation") / 100),
                2,
            ),
        )

        # Trier les r√©sultats
        df_combined = df_combined.orderBy("Code_INSEE_R√©gion", "Ann√©e")

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "‚úÖ Fusion des donn√©es PIB et Inflation r√©ussie :", df_combined, 10
        )

    def transform_technologie_data(self, df):
        """
        Transforme les donn√©es de technologie.

        :param df: DataFrame PySpark brut
        :return: DataFrame PySpark transform√©
        """
        if df is None:
            logger.error("‚ùå Le DataFrame technologie est vide ou invalide.")
            return None

        logger.info("üöÄ Transformation des donn√©es de technologie en cours...")

        try:
            return self._extracted_from_transform_technologie_data_15(df)
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la transformation des donn√©es : {str(e)}")
            return None

    # TODO Rename this here and in `transform_technologie_data`
    def _extracted_from_transform_technologie_data_15(self, df):
        df_transformed = self._select_and_rename_columns(df)
        df_transformed = self._round_percentages(df_transformed)
        df_transformed = self._clean_years(df_transformed)
        df_transformed = self._replace_nan_year(df_transformed)

        logger.info("‚úÖ Transformation des donn√©es de technologie r√©ussie")
        return df_transformed

    def _select_and_rename_columns(self, df):
        """S√©lectionne et renomme les colonnes."""
        return df.select(
            col("_c0").alias("annee").cast("string"),
            col("DIRD/PIB  France").alias("dird_pib_france_pourcentages").cast("float"),
        )

    def _round_percentages(self, df):
        """Arrondit les pourcentages √† 2 d√©cimales."""
        return df.withColumn(
            "dird_pib_france_pourcentages",
            round(col("dird_pib_france_pourcentages"), 2),
        )

    def _clean_years(self, df):
        """Nettoie les ann√©es en supprimant '.0'."""
        return df.withColumn("annee", regexp_replace("annee", "\.0", ""))

    def _replace_nan_year(self, df):
        """Remplace les valeurs 'NaN' dans la colonne 'annee' par '2023'."""
        return df.withColumn(
            "annee", when(col("annee") == "NaN", "2023").otherwise(col("annee"))
        )

    def transform_election_data_1965_2012(self, list_df):
        """
        Transforme et agr√®ge les fichiers CSV 1965-2012.
        - Unpivot via STACK
        - Agr√©gation par candidat
        - S√©lection du candidat gagnant par d√©partement et ann√©e
        - Nettoyage du nom du candidat
        :param list_df: liste de DataFrames bruts
        :return: DataFrame PySpark final (ann√©e, code_dept, candidat, total_voix)
        """
        if not list_df:
            logger.warning("Liste de DataFrames 1965-2012 vide.")
            return None
        results = []
        # Boucle sur chacun des DF (un par fichier CSV)
        for df in list_df:
            # Colonnes cl√©s (√† ne pas unpivoter)
            key_columns = {
                "Code d√©partement",
                "Code d√©partement0",
                "Code d√©partement1",
                "d√©partement",
                "circonscription",
                "Inscrits",
                "Votants",
                "Exprim√©s",
                "Blancs et nuls",
                "filename",
                "annee",
            }

            # D√©termine la bonne colonne de d√©partement
            if "Code d√©partement" in df.columns:
                dept_col = "Code d√©partement"
            elif "Code d√©partement0" in df.columns:
                dept_col = "Code d√©partement0"
            else:
                # Pas de colonne attendue
                continue

            # Colonnes candidats
            candidate_columns = [c for c in df.columns if c not in key_columns]
            n = len(candidate_columns)
            if n == 0:
                continue

            # Expression stack pour unpivot
            expr_parts = []
            for c in candidate_columns:
                escaped_col = c.replace("'", "''")
                expr_parts.append(f"'{escaped_col}', cast(`{c}` as int)")
            expr = f"stack({n}, {', '.join(expr_parts)}) as (candidat, voix)"

            # Unpivot
            df_unpivot = df.select(
                "annee", F.col(dept_col).alias("code_dept"), F.expr(expr)
            )

            # Agr√©gation par d√©partement / candidat / ann√©e
            df_agg = df_unpivot.groupBy("annee", "code_dept", "candidat").agg(
                F.sum("voix").alias("total_voix")
            )

            # S√©lection du gagnant par dept + ann√©e
            windowSpec = Window.partitionBy("annee", "code_dept").orderBy(
                F.desc("total_voix")
            )
            df_winner = df_agg.withColumn(
                "rank", F.row_number().over(windowSpec)
            ).filter(F.col("rank") == 1)

            # Nettoyage du nom du candidat
            df_winner = df_winner.withColumn("gagnant", F.col("candidat"))
            df_winner = df_winner.withColumn(
                "gagnant", F.trim(F.regexp_replace(F.col("gagnant"), r"\([^)]*\)", ""))
            )
            df_winner = df_winner.withColumn(
                "gagnant",
                when(F.col("gagnant") == "SARKOZY", "Nicolas SARKOZY")
                .when(F.col("gagnant") == "CHIRAC", "Jacques CHIRAC")
                .when(F.col("gagnant") == "MITTERRAND", "Fran√ßois MITTERRAND")
                .when(F.col("gagnant") == "DE GAULLE", "Charles DE GAULLE")
                .when(F.col("gagnant") == "GISCARD DESTAING", "Val√©ry GISCARD DESTAING")
                .when(F.col("gagnant") == "POMPIDOU", "Georges POMPIDOU")
                .when(F.col("gagnant") == "POHER", "Alain POHER")
                .when(F.col("gagnant") == "JOSPIN", "Lionel JOSPIN")
                .when(F.col("gagnant") == "ROYAL", "S√©gol√®ne ROYAL")
                .when(F.col("gagnant") == "HOLLANDE", "Fran√ßois HOLLANDE")
                .when(F.col("gagnant") == "MACRON", "Emmanuel MACRON")
                .when(F.col("gagnant") == "LE PEN", "Marine LE PEN")
                .otherwise(F.col("gagnant")),
            )

            # S√©lection colonnes finales
            results.append(
                df_winner.select(
                    "annee",
                    "code_dept",
                    F.col("gagnant").alias("candidat"),
                    "total_voix",
                )
            )

        # Union de tous les r√©sultats
        if results:
            final_df = results[0]
            for df_r in results[1:]:
                final_df = final_df.union(df_r)

            # Normalisation du code_dept (ex: passer '1' -> '01')
            final_df = (
                final_df.withColumn(
                    "code_dept_norm",
                    F.when(
                        F.col("code_dept").rlike("^[0-9]$"),
                        F.lpad(F.col("code_dept"), 2, "0"),
                    ).otherwise(F.col("code_dept")),
                )
                .drop("code_dept")
                .withColumnRenamed("code_dept_norm", "code_dept")
            )

            return final_df

        else:
            logger.warning("Aucune donn√©e agr√©g√©e pour 1965-2012.")
            return None

    def transform_election_data_2017(self, df_2017_raw):
        """
        Transforme le fichier Excel 2017 :
        - S√©lection du candidat gagnant par d√©partement
        - Nettoyage (codes sp√©ciaux pour r√©gions d outre-mer)
        """
        if df_2017_raw is None:
            logger.warning("DataFrame 2017 vide.")
            return None

        df_2017 = (
            df_2017_raw.withColumnRenamed("Code du d√©partement", "code_dept")
            .withColumn(
                "candidat1", F.concat(F.col("Nom17"), F.lit(" "), F.col("Pr√©nom18"))
            )
            .withColumn(
                "candidat2", F.concat(F.col("Nom23"), F.lit(" "), F.col("Pr√©nom24"))
            )
            .select(
                F.col("code_dept").cast("string"),
                F.col("Libell√© du d√©partement"),
                F.col("Voix19").alias("voix1").cast("int"),
                F.col("Voix25").alias("voix2").cast("int"),
                "candidat1",
                "candidat2",
            )
        )

        # On cr√©e un DataFrame par candidat
        df_2017_candidate1 = df_2017.select(
            "code_dept",
            F.col("candidat1").alias("candidat"),
            F.col("voix1").alias("voix"),
            F.col("Libell√© du d√©partement"),
        )

        df_2017_candidate2 = df_2017.select(
            "code_dept",
            F.col("candidat2").alias("candidat"),
            F.col("voix2").alias("voix"),
            F.col("Libell√© du d√©partement"),
        )

        # Union des deux candidats
        df_2017_norm = df_2017_candidate1.union(df_2017_candidate2).withColumn(
            "annee", F.lit("2017")
        )

        # 1. Appliquer le mapping pour les codes sp√©ciaux et la Corse
        df_2017_norm = df_2017_norm.withColumn(
            "code_dept_norm",
            F.when(F.col("Libell√© du d√©partement") == "Guadeloupe", "ZA")
            .when(F.col("Libell√© du d√©partement") == "Martinique", "ZB")
            .when(F.col("Libell√© du d√©partement") == "Guyane", "ZC")
            .when(F.col("Libell√© du d√©partement") == "La R√©union", "ZD")
            .when(F.col("Libell√© du d√©partement") == "Mayotte", "ZM")
            .when(F.col("Libell√© du d√©partement") == "Nouvelle-Cal√©donie", "ZN")
            .when(F.col("Libell√© du d√©partement") == "Polyn√©sie fran√ßaise", "ZP")
            .when(F.col("Libell√© du d√©partement") == "Saint-Pierre-et-Miquelon", "ZS")
            .when(
                F.col("Libell√© du d√©partement") == "Saint-Martin/Saint-Barth√©lemy", "ZX"
            )
            .when(F.col("Libell√© du d√©partement") == "Wallis et Futuna", "ZW")
            .when(
                F.col("Libell√© du d√©partement") == "Fran√ßais √©tablis hors de France",
                "ZZ",
            )
            .when(F.col("Libell√© du d√©partement") == "Corse-du-Sud", "2A")
            .when(F.col("Libell√© du d√©partement") == "Haute-Corse", "2B")
            .otherwise(F.col("code_dept")),
        )

        # 1. Supprimer la terminaison ".0" dans la colonne "code_dept_norm"
        df_final_2017 = df_2017_norm.withColumn(
            "code_dept_final", F.regexp_replace(F.col("code_dept_norm"), r"\.0$", "")
        )

        # 2. (Optionnel) Si vous souhaitez que les codes sur un seul chiffre soient affich√©s sur 2 chiffres (ex. "1" -> "01")
        df_final_2017 = df_final_2017.withColumn(
            "code_dept_final",
            F.when(
                F.col("code_dept_final").rlike("^[0-9]$"),
                F.lpad(F.col("code_dept_final"), 2, "0"),
            ).otherwise(F.col("code_dept_final")),
        )

        # 3. Supprimer les colonnes interm√©diaires et renommer la colonne finale en "code_dept"
        df_final_2017 = df_final_2017.drop(
            "code_dept", "code_dept_norm", "Libell√© du d√©partement"
        ).withColumnRenamed("code_dept_final", "code_dept")

        # Pour chaque d√©partement, on garde le candidat avec le maximum de voix
        w_dept = Window.partitionBy("annee", "code_dept").orderBy(F.desc("voix"))
        return (
            df_final_2017.withColumn("rank", F.row_number().over(w_dept))
            .filter(F.col("rank") == 1)
            .select("annee", "code_dept", "candidat", "voix")
        )

    def transform_election_data_2022(self, df_2022_raw):
        """
        Transforme le fichier Excel 2022 :
        - S√©lection du gagnant par d√©partement
        - Normalisation du nom du candidat (Emmanuel MACRON, Marine LE PEN, etc.)
        """
        if df_2022_raw is None:
            logger.warning("DataFrame 2022 vide.")
            return None

        # Pour 2022, on suppose que chaque ligne correspond d√©j√† √† un candidat,
        # avec "Code du d√©partement", "Nom", "Pr√©nom" et "Voix".
        df_2022 = (
            df_2022_raw.withColumnRenamed("Code du d√©partement", "code_dept")
            .withColumn("candidat", F.concat(F.col("Nom"), F.lit(" "), F.col("Pr√©nom")))
            .select(
                F.col("code_dept").cast("string"),
                "candidat",
                F.col("Voix").alias("voix"),
            )
            .withColumn("annee", F.lit("2022"))
        )

        # On agr√®ge par d√©partement pour s√©lectionner le candidat gagnant (le plus de voix)
        w_dept_2022 = Window.partitionBy("annee", "code_dept").orderBy(F.desc("voix"))
        return (
            df_2022.withColumn("rank", F.row_number().over(w_dept_2022))
            .filter(F.col("rank") == 1)
            .select("annee", "code_dept", "candidat", "voix")
        )

    def combine_all_years(self, df_1965_2012, df_2017, df_2022):
        """
        Combine les DataFrames de 1965-2012, 2017 et 2022.
        Applique les mappings DOM-TOM (ZA->971 etc.) et renomme la colonne 'voix' en 'total_voix'.
        """
        if df_1965_2012 is None and df_2017 is None and df_2022 is None:
            logger.warning("Aucun DataFrame √† combiner.")
            return None

        # Union 2017 et 2022
        df_final = df_2017.union(df_2022)

        # 1. Appliquer le mapping pour les codes sp√©ciaux
        df_final = df_final.withColumn(
            "code_dept",
            F.when(F.col("code_dept") == "ZA", "971")
            .when(F.col("code_dept") == "ZB", "972")
            .when(F.col("code_dept") == "ZC", "973")
            .when(F.col("code_dept") == "ZD", "974")
            .when(F.col("code_dept") == "ZM", "976")
            .when(F.col("code_dept") == "ZN", "988")
            .when(F.col("code_dept") == "ZP", "987")
            .when(F.col("code_dept") == "ZS", "975")
            .when(F.col("code_dept") == "ZX", "971")
            .when(F.col("code_dept") == "ZW", "986")
            .when(F.col("code_dept") == "ZZ", "99")
            .otherwise(F.col("code_dept")),
        )

        # 1.1 Normalisation des candidats avec pr√©nom et nom
        df_final = df_final.withColumn(
            "candidat",
            F.when(F.col("candidat") == "MACRON Emmanuel", "Emmanuel MACRON").when(
                F.col("candidat") == "LE PEN Marine", "Marine LE PEN"
            ),
        )

        # 2. Appliquer le format int pour les voix
        df_final = df_final.withColumn("voix", F.col("voix").cast("int"))

        # 3. Renommer la colonne "voix" en "total_voix"
        df_final = df_final.withColumnRenamed("voix", "total_voix")

        # 4. S√©lection des colonnes d'int√©r√™t
        df_1965_2012 = df_1965_2012.select(
            "annee", "code_dept", "candidat", "total_voix"
        )

        # 5. Union des deux DataFrames
        df_final_csv = df_final.union(df_1965_2012)

        # Tri final
        df_final_csv = df_final_csv.orderBy("annee", "code_dept")

        return df_final_csv

    def transform_life_expectancy_data(self, df_life, df_departments):
        """
        Transforme les donn√©es d'esp√©rance de vie √† la naissance pour hommes et femmes pour
        obtenir une colonne unique "Esp√©rance_Vie" correspondant √† la moyenne des valeurs hommes et femmes.

        √âtapes :
        - Filtrage des lignes dont le libell√© commence par "Esp√©rance de vie √† la naissance - Hommes" ou "Esp√©rance de vie √† la naissance - Femmes"
        - Extraction du genre et du nom de d√©partement ou r√©gion depuis le libell√©
        - S√©lection des colonnes des ann√©es (2000 √† 2022) et conversion du format large en format long via STACK
        - Pivot sur la colonne "Genre" pour obtenir deux colonnes ("Hommes" et "Femmes")
        - Normalisation des noms pour ne conserver que les d√©partements r√©els (√† l'aide du DataFrame des d√©partements)
        - Jointure avec le DataFrame des d√©partements pour r√©cup√©rer le code d√©partement r√©el (CODE_DEP)
        - Calcul de la moyenne de "Hommes" et "Femmes" et cr√©ation d'une colonne unique "Esp√©rance_Vie"

        :param df_life: DataFrame PySpark contenant les donn√©es brutes d'esp√©rance de vie.
        :param df_departments: DataFrame PySpark contenant les donn√©es des d√©partements.
        :return: DataFrame final avec colonnes CODE_DEP, Ann√©e, Esp√©rance_Vie.
        """
        if df_life is None:
            logger.error("‚ùå Le DataFrame d'esp√©rance de vie est vide ou invalide.")
            return None

        logger.info("üöÄ Transformation des donn√©es d'esp√©rance de vie en cours...")

        # Filtrer les lignes d'int√©r√™t
        df_filtered = df_life.filter(
            (col("Libell√©").rlike("^Esp√©rance de vie √† la naissance - Hommes"))
            | (col("Libell√©").rlike("^Esp√©rance de vie √† la naissance - Femmes"))
        )

        # Extraire le genre et le "nom de d√©partement ou r√©gion" depuis le libell√©
        df_filtered = df_filtered.withColumn(
            "Genre",
            regexp_extract(
                col("Libell√©"),
                r"Esp√©rance de vie √† la naissance - (Hommes|Femmes) - (.*)",
                1,
            ),
        ).withColumn(
            "D√©partement",
            trim(
                regexp_extract(
                    col("Libell√©"),
                    r"Esp√©rance de vie √† la naissance - (Hommes|Femmes) - (.*)",
                    2,
                )
            ),
        )

        # S√©lectionner les colonnes des ann√©es de 2000 √† 2022
        years = [str(year) for year in range(2000, 2023)]
        selected_cols = ["Libell√©", "Genre", "D√©partement"] + years
        df_selected = df_filtered.select(*selected_cols)

        # Conversion du format large en format long via STACK
        n_years = len(years)
        stack_expr = "stack({0}, {1}) as (Annee, Esperance_de_vie)".format(
            n_years, ", ".join([f"'{year}', `{year}`" for year in years])
        )
        df_long = df_selected.select("Genre", "D√©partement", expr(stack_expr))
        df_long = df_long.withColumn(
            "Annee", col("Annee").cast(IntegerType())
        ).withColumn("Esperance_de_vie", col("Esperance_de_vie").cast(DoubleType()))
        df_long = df_long.filter(col("Annee").between(2000, 2022))

        # Pivot pour cr√©er des colonnes pour Hommes et Femmes
        df_pivot = (
            df_long.groupBy("D√©partement", "Annee")
            .pivot("Genre", ["Hommes", "Femmes"])
            .agg(F.first("Esperance_de_vie"))
        )

        # Fonction de normalisation des noms
        def normalize_dept(column):
            norm = F.lower(trim(column))
            # Remplacer les accents par leurs √©quivalents non accentu√©s
            norm = F.translate(norm, "√©√®√™√´√†√¢√§√Æ√Ø√¥√∂√π√ª√º√ß", "eeeeaaaiioouuuc")
            # Supprimer tirets, apostrophes et espaces
            norm = F.regexp_replace(norm, "[-' ]", "")
            return norm

        # Appliquer la normalisation sur le DataFrame pivot√©
        df_pivot = df_pivot.withColumn(
            "D√©partement_norm", normalize_dept(col("D√©partement"))
        )
        # Normaliser le DataFrame des d√©partements
        df_depts_norm = df_departments.withColumn(
            "nom_departement_norm", normalize_dept(col("nom_departement"))
        )

        # Filtrage : ne conserver que les lignes correspondant √† des d√©partements r√©els
        valid_dept_names = [
            row["nom_departement_norm"]
            for row in df_depts_norm.select("nom_departement_norm").distinct().collect()
        ]
        logger.info(
            "Liste des d√©partements valides (normalis√©s) : "
            + ", ".join(valid_dept_names)
        )
        df_pivot = df_pivot.filter(col("D√©partement_norm").isin(valid_dept_names))

        # Jointure pour associer le code de d√©partement r√©el
        df_joined = df_pivot.join(
            df_depts_norm,
            df_pivot["D√©partement_norm"] == df_depts_norm["nom_departement_norm"],
            "left",
        )

        # Calcul de la moyenne des deux colonnes pour obtenir une seule colonne "Esp√©rance_Vie"
        df_final = df_joined.select(
            df_depts_norm["code_departement"].alias("CODE_DEP"),
            col("Annee").alias("Ann√©e"),
            round(((col("Hommes") + col("Femmes")) / 2), 2).alias("Esp√©rance_Vie"),
        ).orderBy("CODE_DEP", "Ann√©e")

        logger.info("‚úÖ Transformation termin√©e ! Aper√ßu :")
        df_final.show(10, truncate=False)

        # Affichage de d√©bogage : lignes non associ√©es (si besoin)
        df_unmatched = df_joined.filter(df_depts_norm["code_departement"].isNull())
        logger.info("Lignes non associ√©es apr√®s jointure :")
        df_unmatched.select("D√©partement", "D√©partement_norm").distinct().show(
            truncate=False
        )

        return df_final

    def fill_missing_mayotte_life_expectancy(self, df_final):
        """
        Compl√®te les valeurs manquantes pour Mayotte (CODE_DEP = "976")
        dans le DataFrame final en utilisant une r√©gression lin√©aire sur l'ann√©e.
        On entra√Æne un mod√®le sur les donn√©es connues (colonne 'Esp√©rance_Vie')
        puis on pr√©dit pour les ann√©es manquantes.

        :param df_final: DataFrame final avec colonnes CODE_DEP, Ann√©e, Esp√©rance_Vie
        :return: DataFrame final avec les valeurs manquantes pour Mayotte compl√©t√©es
                et arrondies √† 2 d√©cimales.
        """
        from pyspark.ml.feature import VectorAssembler
        from pyspark.ml.regression import LinearRegression
        from pyspark.sql.functions import col, when, round

        # Filtrer uniquement les donn√©es de Mayotte
        df_mayotte = df_final.filter(col("CODE_DEP") == "976")

        # S√©parer les donn√©es connues et inconnues pour la colonne "Esp√©rance_Vie"
        known = df_mayotte.filter(col("Esp√©rance_Vie").isNotNull())
        unknown = df_mayotte.filter(col("Esp√©rance_Vie").isNull())

        # Pr√©parer les donn√©es pour la r√©gression
        assembler = VectorAssembler(inputCols=["Ann√©e"], outputCol="features")
        train = assembler.transform(known).select("Ann√©e", "features", "Esp√©rance_Vie")

        # Entra√Æner le mod√®le de r√©gression lin√©aire
        lr = LinearRegression(featuresCol="features", labelCol="Esp√©rance_Vie")
        model = lr.fit(train)

        # Pr√©dire pour les ann√©es manquantes
        pred = assembler.transform(unknown)
        pred = model.transform(pred).select(
            "Ann√©e", col("prediction").alias("pred_value")
        )

        # Remplacer les valeurs manquantes par la pr√©diction (arrondie √† 2 d√©cimales)
        df_mayotte_filled = (
            df_mayotte.alias("base")
            .join(pred.alias("pred"), on="Ann√©e", how="left")
            .withColumn(
                "Esp√©rance_Vie_new",
                when(
                    col("base.Esp√©rance_Vie").isNull(), round(col("pred.pred_value"), 2)
                ).otherwise(round(col("base.Esp√©rance_Vie"), 2)),
            )
            .select(
                col("base.CODE_DEP").alias("CODE_DEP"),
                col("base.Ann√©e").alias("Ann√©e"),
                col("Esp√©rance_Vie_new").alias("Esp√©rance_Vie"),
            )
        )

        # Conserver les donn√©es des autres d√©partements
        df_other = df_final.filter(col("CODE_DEP") != "976")

        # Fusionner et trier le DataFrame final
        return df_other.unionByName(df_mayotte_filled).orderBy("CODE_DEP", "Ann√©e")

    def transform_education_data(self, df):
        """
        Transforme et nettoie les donn√©es d'√©ducation issues du CSV 'fr-en-etablissements-fermes.csv'.
        √âtapes de transformation :
          1. Suppression des doublons.
          2. Standardisation de toutes les colonnes textuelles (conversion en minuscules, suppression des espaces,
             remplacement des valeurs nulles par "non sp√©cifi√©").
          3. Conversion de la colonne "date_fermeture" en type Date et extraction de l'ann√©e dans "annee_fermeture".
          4. Normalisation du code postal : remplacement des valeurs nulles par "00000", puis suppression des espaces.
          5. S√©paration des secteurs public et priv√© √† partir de la colonne "secteur_public_prive_libe".
        :param df: DataFrame Spark brut issu du fichier CSV d'√©ducation.
        :return: DataFrame nettoy√© et transform√©.
        """

        logger.info("üöÄ Transformation des donn√©es d'√©ducation en cours...")

        # 1. Suppression des doublons
        df = df.dropDuplicates()

        # 2. Standardisation des colonnes textuelles
        for column in df.columns:
            df = df.withColumn(
                column,
                F.when(
                    F.col(column).isNotNull(), F.trim(F.lower(F.col(column)))
                ).otherwise(F.lit("non sp√©cifi√©")),
            )

        # 3. Conversion de 'date_fermeture' en DateType et extraction de l'ann√©e
        if "date_fermeture" in df.columns:
            df = df.withColumn(
                "date_fermeture", F.col("date_fermeture").cast(DateType())
            )
            df = df.withColumn("annee_fermeture", F.year(F.col("date_fermeture")))

        # 4. Normalisation du code postal
        if "code_postal" in df.columns:
            df = df.withColumn(
                "code_postal",
                F.when(F.col("code_postal").isNull(), F.lit("00000")).otherwise(
                    F.trim(F.col("code_postal"))
                ),
            )

        # 5. S√©paration du secteur public/priv√©
        if "secteur_public_prive_libe" in df.columns:
            df = df.withColumn(
                "secteur_public",
                F.when(F.col("secteur_public_prive_libe") == "public", 1).otherwise(0),
            )
            df = df.withColumn(
                "secteur_prive",
                F.when(F.col("secteur_public_prive_libe") == "priv√©", 1).otherwise(0),
            )

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "‚úÖ Transformation des donn√©es d'√©ducation r√©ussie.", df, 5
        )

    def calculate_closed_by_year_and_dept_education(self, df):
        """
        Calcule le nombre d'√©tablissements ferm√©s par ann√©e et par d√©partement √† partir des donn√©es d'√©ducation.
        Regroupe par 'annee_fermeture', 'code_departement' et 'libelle_departement', puis agr√®ge :
        - Le nombre total d'√©tablissements (count sur "numero_uai"),
        - Le nombre d'√©tablissements ferm√©s dans le secteur public (sum de "secteur_public"),
        - Le nombre dans le secteur priv√© (sum de "secteur_prive"),
        - Les pourcentages correspondants (arrondis √† 2 d√©cimales).

        Ensuite, pour chaque d√©partement pr√©sent, les combinaisons manquantes pour les ann√©es cibles
        (2002, 2007, 2012, 2017, 2022) sont compl√©t√©es avec des valeurs par d√©faut (0).

        :param df: DataFrame nettoy√© d'√©ducation, incluant les colonnes "annee_fermeture",
                "code_departement", "libelle_departement", "numero_uai", "secteur_public" et "secteur_prive".
        :return: DataFrame avec les statistiques par ann√©e et d√©partement compl√©t√©es.
        """
        import pyspark.sql.functions as F
        from pyspark.sql.functions import col, lit, when

        logger.info(
            "üöÄ Calcul des statistiques de fermetures d'√©tablissements par d√©partement et ann√©e..."
        )

        # Agr√©gation initiale - √©viter le regroupement par libelle_departement pour r√©duire la m√©moire
        df_grouped = df.groupBy("annee_fermeture", "code_departement").agg(
            F.first("libelle_departement").alias("libelle_departement"),
            F.count("numero_uai").alias("nombre_total_etablissements"),
            F.sum("secteur_public").alias("nb_public"),
            F.sum("secteur_prive").alias("nb_prive"),
        )

        # Calculer les pourcentages avec une expression s√©curis√©e pour √©viter division par z√©ro
        df_grouped = df_grouped.withColumn(
            "pct_public",
            F.round(
                F.when(
                    F.col("nombre_total_etablissements") > 0,
                    F.col("nb_public") * 100.0 / F.col("nombre_total_etablissements"),
                ).otherwise(0.0),
                2,
            ),
        ).withColumn(
            "pct_prive",
            F.round(
                F.when(
                    F.col("nombre_total_etablissements") > 0,
                    F.col("nb_prive") * 100.0 / F.col("nombre_total_etablissements"),
                ).otherwise(0.0),
                2,
            ),
        )

        # Liste des ann√©es cibles pour lesquelles on souhaite forcer une pr√©sence
        target_years = [2002, 2007, 2012, 2017, 2022]

        # R√©cup√©rer uniquement les d√©partements uniques pour √©viter de multiplier les donn√©es
        df_depts = (
            df.select("code_departement", "libelle_departement").distinct().cache()
        )

        # Cr√©er des donn√©es pour les ann√©es manquantes pour chaque d√©partement
        result_dfs = []

        for year in target_years:
            # Pour chaque ann√©e cible, cr√©er un DataFrame avec cette ann√©e et tous les d√©partements
            df_year = df_depts.withColumn("annee_fermeture", lit(year))

            # Jointure gauche avec les donn√©es existantes
            df_year_completed = df_year.join(
                df_grouped.filter(F.col("annee_fermeture") == year),
                on=["code_departement", "annee_fermeture", "libelle_departement"],
                how="left",
            )

            # Remplir les valeurs manquantes
            df_year_completed = df_year_completed.na.fill(
                {
                    "nombre_total_etablissements": 0,
                    "nb_public": 0,
                    "nb_prive": 0,
                    "pct_public": 0.0,
                    "pct_prive": 0.0,
                }
            )

            result_dfs.append(df_year_completed)

        # Union de tous les r√©sultats par ann√©e avec les donn√©es originales filtr√©es sur les ann√©es non-cibles
        df_other_years = df_grouped.filter(~F.col("annee_fermeture").isin(target_years))
        result_dfs.append(df_other_years)

        # Effectuer l'union de tous les DataFrames
        df_completed = result_dfs[0]
        for i in range(1, len(result_dfs)):
            df_completed = df_completed.unionByName(
                result_dfs[i], allowMissingColumns=True
            )

        # Nettoyer les valeurs nulles qui pourraient rester dans les colonnes num√©riques
        df_completed = df_completed.na.fill(
            {
                "nombre_total_etablissements": 0,
                "nb_public": 0,
                "nb_prive": 0,
                "pct_public": 0.0,
                "pct_prive": 0.0,
            }
        )

        # Tri final par ann√©e et d√©partement
        df_completed = df_completed.orderBy("annee_fermeture", "code_departement")

        # Lib√©rer la m√©moire du cache
        df_depts.unpersist()

        return self._extracted_from__extracted_from_combine_election_and_orientation_politique_52_116(
            "‚úÖ Calcul des statistiques compl√©t√©. Aper√ßu :", df_completed, 10
        )

    def transform_security_data(self, df):
        """
        Transforme les donn√©es de s√©curit√© exclusivement avec pandas.
        Format de sortie: Ann√©e, D√©partement, D√©lits_total avec s√©parateur virgule.

        :param df: Non utilis√© dans cette version
        :return: DataFrame pandas avec colonnes (Ann√©e, D√©partement, D√©lits_total)
        """
        import pandas as pd
        from collections import defaultdict
        import os

        logger.info(
            "üöÄ Transformation des donn√©es de s√©curit√© avec pandas uniquement..."
        )

        try:
            # Chemin du fichier Excel
            fichier_excel = "data/securite/tableaux-4001-ts.xlsx"

            if not os.path.exists(fichier_excel):
                logger.error(f"‚ùå Fichier Excel non trouv√©: {fichier_excel}")
                return None

            logger.info(f"üìÇ Chargement du fichier Excel: {fichier_excel}")
            xls = pd.ExcelFile(fichier_excel)

            # Liste des feuilles correspondant aux d√©partements
            departements = [sheet for sheet in xls.sheet_names if sheet.isdigit()]
            logger.info(f"‚úì {len(departements)} d√©partements identifi√©s")

            # Dictionnaire pour stocker les r√©sultats {(d√©partement, ann√©e): total}
            resultats = defaultdict(int)

            # Plage d'ann√©es cibl√©e
            annees_cibles = set(range(1996, 2023))  # de 1996 √† 2022 inclus

            # Traitement de chaque feuille
            for dept in departements:
                try:
                    resultats = self._process_department_sheet(dept, xls, annees_cibles, resultats)
                except Exception as e:
                    logger.error(f"‚ùå Erreur sur le d√©partement {dept}: {str(e)}")
                    
            # Cr√©ation du DataFrame final
            df_final = self._create_final_security_dataframe(resultats)

            logger.info(
                "‚úÖ Transformation des donn√©es de s√©curit√© termin√©e avec succ√®s"
            )
            return df_final

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la transformation: {str(e)}")
            import traceback

            logger.error(traceback.format_exc())
            return None
            
    def _process_department_sheet(self, dept, xls, annees_cibles, resultats):
        """
        Traite une feuille de d√©partement et met √† jour le dictionnaire des r√©sultats.
        
        :param dept: Code du d√©partement
        :param xls: Objet ExcelFile pandas
        :param annees_cibles: Ensemble des ann√©es √† consid√©rer
        :param resultats: Dictionnaire des r√©sultats √† mettre √† jour
        :return: Dictionnaire des r√©sultats mis √† jour
        """
        import pandas as pd
        
        logger.info(f"‚ú® Traitement du d√©partement {dept}...")
        df_dept = xls.parse(dept)
        df_dept = df_dept.dropna(how="all")  # retirer les lignes totalement vides

        for col in df_dept.columns:
            if isinstance(col, str) and col.startswith("_"):
                try:
                    annee = int(col.split("_")[1])
                    if annee in annees_cibles:
                        resultats[(annee, dept)] += df_dept[col].sum(skipna=True)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Probl√®me avec la colonne {col}: {str(e)}")
                    
        return resultats
        
    def _create_final_security_dataframe(self, resultats):
        """
        Cr√©e le DataFrame final √† partir du dictionnaire des r√©sultats.
        
        :param resultats: Dictionnaire {(ann√©e, d√©partement): total}
        :return: DataFrame pandas format√©
        """
        import pandas as pd
        
        logger.info("‚úì Cr√©ation du DataFrame final...")
        df_final = pd.DataFrame(
            [
                {"Ann√©e": annee, "D√©partement": dept, "D√©lits_total": int(total)}
                for (annee, dept), total in resultats.items()
            ]
        )

        # Formater le code d√©partement pour avoir toujours 2 chiffres
        df_final["D√©partement"] = df_final["D√©partement"].apply(
            lambda x: x.zfill(2) if x.isdigit() and len(x) == 1 else x
        )

        # Tri pour lisibilit√© (d'abord par ann√©e, puis par d√©partement)
        df_final = df_final.sort_values(by=["Ann√©e", "D√©partement"])

        # Afficher des statistiques
        if not df_final.empty:
            logger.info(
                f"‚úì P√©riode couverte: de {df_final['Ann√©e'].min()} √† {df_final['Ann√©e'].max()}"
            )
            logger.info(
                f"‚úì {df_final['D√©partement'].nunique()} d√©partements trait√©s"
            )
            logger.info(f"‚úì Total de {len(df_final)} lignes de donn√©es g√©n√©r√©es")

            # √âchantillon des donn√©es
            logger.info(
                "‚úì √âchantillon des donn√©es (format: Ann√©e,D√©partement,D√©lits_total):"
            )
            for idx, row in df_final.head(5).iterrows():
                logger.info(
                    f"{row['Ann√©e']},{row['D√©partement']},{int(row['D√©lits_total'])}"
                )
                
        return df_final

    def transform_demography_data(self, df):
        """
        Transforme les donn√©es d√©mographiques issues du CSV en :
        - Renommant les colonnes principales
        - Nettoyant la colonne du code d√©partement et en filtrant les lignes parasites
        - Conserver la colonne 'Ann√©e' (provenant du nom de la feuille Excel)
        - Classer par Ann√©e, puis par d√©partement
        """
        from pyspark.sql.functions import col, trim, regexp_replace, split, when

        if df is None:
            logger.error("‚ùå Le DataFrame de d√©mographie est vide ou invalide.")
            return None

        logger.info("üöÄ Transformation des donn√©es d√©mographiques en cours...")

        # 1) Renommage des colonnes principales (selon votre CSV)
        #    Assurez-vous que ces noms correspondent √† votre structure r√©elle
        df = (
            df.withColumnRenamed("D√©partements", "Code_D√©partement")
            .withColumnRenamed("Unnamed: 1", "Nom_D√©partement")
            .withColumnRenamed("Ensemble", "E_Total")
            .withColumnRenamed("Hommes", "H_Total")
            .withColumnRenamed("Femmes", "F_Total")
        )

        # 2) Renommage des colonnes des tranches d'√¢ge
        df = (
            df.withColumnRenamed("Unnamed: 3", "E_0_19_ans")
            .withColumnRenamed("Unnamed: 4", "E_20_39_ans")
            .withColumnRenamed("Unnamed: 5", "E_40_59_ans")
            .withColumnRenamed("Unnamed: 6", "E_60_74_ans")
            .withColumnRenamed("Unnamed: 7", "E_75_et_plus")
        )

        # 3) Filtrer les lignes parasites
        #    (celles qui commencent par "Source", contiennent "France" ou "DOM", etc.)
        #    et aussi la ligne d'en-t√™te r√©p√©t√©e (rep√©r√©e par "0 √† 19 ans" dans E_0_19_ans)
        df = df.filter(
            ~col("Code_D√©partement").startswith("Source")
            & ~col("Code_D√©partement").contains("France")
            & ~col("Code_D√©partement").contains("DOM")
            & ~col("Code_D√©partement").startswith("NB")
            & ~col("Code_D√©partement").startswith("Population")
            & (col("E_0_19_ans") != "0 √† 19 ans")
        )

        # 4) Nettoyer la colonne Code_D√©partement : suppression des espaces et guillemets
        df = df.withColumn("Code_D√©partement", trim(col("Code_D√©partement")))
        df = df.withColumn(
            "Code_D√©partement", regexp_replace(col("Code_D√©partement"), '"', "")
        )

        # 5) Extraire le code (premier token) et √©ventuellement le nom depuis la colonne Code_D√©partement
        df = df.withColumn(
            "first_token", split(col("Code_D√©partement"), " ", 2)[0]
        ).withColumn("remainder", split(col("Code_D√©partement"), " ", 2)[1])

        # 6) Ne conserver que les lignes dont le premier token correspond √† un code d√©partement valide
        df = df.filter(col("first_token").rlike("^(2A|2B|[0-9]{1,3})$"))

        # 7) Remplacer Code_D√©partement par le premier token
        df = df.withColumn("Code_D√©partement", col("first_token"))

        # 8) Si Nom_D√©partement est vide, utiliser remainder comme nom
        df = df.withColumn(
            "Nom_D√©partement",
            when(
                (col("Nom_D√©partement").isNull()) | (col("Nom_D√©partement") == ""),
                col("remainder"),
            ).otherwise(col("Nom_D√©partement")),
        )

        # 9) Supprimer les colonnes temporaires
        df = df.drop("first_token", "remainder")

        # 10) S√©lectionner et r√©organiser les colonnes dans l'ordre souhait√©
        #     On inclut d√©sormais "Ann√©e" pour la conserver et trier ensuite.
        final_columns = [
            "Ann√©e",
            "Code_D√©partement",
            "Nom_D√©partement",
            "E_Total",
            "H_Total",
            "F_Total",
            "E_0_19_ans",
            "E_20_39_ans",
            "E_40_59_ans",
            "E_60_74_ans",
            "E_75_et_plus",
        ]

        # V√©rifier que la colonne "Ann√©e" existe bien dans votre DataFrame
        # (au cas o√π la ligne "df['Ann√©e'] = sheet" a bien √©t√© cr√©√©e √† l'extraction)
        available_cols = [c for c in final_columns if c in df.columns]
        df_final = df.select(*available_cols)

        # 11) Classer par Ann√©e, puis par Code_D√©partement
        #     Si Ann√©e est stock√©e en string, on peut la convertir en int si c'est un simple nombre
        df_final = (
            df_final.withColumn("Ann√©e_int", col("Ann√©e").cast("int"))
            .orderBy(col("Ann√©e_int").asc(), col("Code_D√©partement"))
            .drop("Ann√©e_int")
        )

        return self._extracted_from__extracted_from_combine_election_and_orientation_politique_52_116(
            "‚úÖ Transformation des donn√©es d√©mographiques termin√©e", df_final, 5
        )

    def combine_election_and_orientation_politique(self, df_election, df_orientation):
        """
        Combinaison des donn√©es √©lectorales avec les donn√©es d'orientation politique.
        """
        if df_election is None or df_orientation is None:
            logger.error("‚ùå Donn√©es invalides pour la combinaison")

        logger.info(
            "üöÄ Combinaison des donn√©es √©lectorales avec les donn√©es d'orientation politique..."
        )

        # 3. Nettoyer les noms des candidats
        df_election = df_election.withColumn(
            "candidat_clean", trim(upper(col("candidat")))
        )

        # 4. Mapping candidat -> orientation politique
        candidate_to_orientation = {
            "CHARLES DE GAULLE": "droite",
            "FRAN√áOIS MITTERRAND": "gauche",
            "VAL√âRY GISCARD D'ESTAING": "centre droite",
            "VAL√âRY GISCARD DESTAING": "centre droite",
            "VALERY GISCARD D'ESTAING": "centre droite",
            "VALERY GISCARD DESTAING": "centre droite",
            "JACQUES CHIRAC": "droite",
            "LIONEL JOSPIN": "gauche",
            "NICOLAS SARKOZY": "droite",
            "S√âGOL√àNE ROYAL": "gauche",
            "FRAN√áOIS HOLLANDE": "gauche",
            "MARINE LE PEN": "extreme droite",
            "JEAN-LUC M√âLENCHON": "extreme gauche",
            "EMMANUEL MACRON": "centre",
            "ARLETTE LAGUILLER": "extreme gauche",
            "PHILIPPE POUTOU": "extreme gauche",
            "NATHALIE ARTHAUD": "extreme gauche",
            "JEAN-MARIE LE PEN": "extreme droite",
            "BENO√éT HAMON": "gauche",
            "DOMINIQUE DE VILLEPIN": "droite",
            "CHRISTINE BOUTIN": "droite",
            "FRAN√áOIS BAYROU": "centre droite",
            "NICOLAS DUPONT-AIGNAN": "droite",
            "√âRIC ZEMMOUR": "extreme droite",
            "YANNICK JADOT": "√©cologiste",
            "NO√ãL MAM√àRE": "√©cologiste",
            "ANTOINE WAQUIN": "extreme gauche",
            "GEORGES MARCHAIS": "gauche",
            "ROBERT HUE": "gauche",
            "GEORGES POMPIDOU": "droite",
            "ALAIN POHER": "centre droite",
        }

        # 5. Ajouter la colonne orientation politique
        orientation_expr = create_map(
            [lit(k) for k in chain(*candidate_to_orientation.items())]
        )
        df_election = df_election.withColumn(
            "orientation_politique", orientation_expr.getItem(col("candidat_clean"))
        )

        # 6. Cr√©er le mapping orientation -> id √† partir du fichier des partis
        orientation_id_map = {
            row["Orientation politique"]: row["id"]
            for row in df_orientation.select("Orientation politique", "id")
            .distinct()
            .collect()
        }
        orientation_id_expr = create_map(
            [lit(k) for k in chain(*orientation_id_map.items())]
        )
        df_election = df_election.withColumn(
            "id_parti", orientation_id_expr.getItem(col("orientation_politique"))
        )

        # Drop candidat_clean
        df_election = df_election.drop("candidat_clean")

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "‚úÖ Combinaison des donn√©es √©lectorales avec les donn√©es d'orientation politique termin√©e",
            df_election,
            5,
        )
    
    def prepare_dim_politique(self, df_election):
        """
        Pr√©pare les donn√©es pour la dimension politique.
        
        Args:
            df_election: DataFrame des donn√©es √©lectorales combin√©es avec l'orientation politique
        
        Returns:
            DataFrame format√© pour dim_politique
        """
        logger.info("üîÑ Pr√©paration des donn√©es pour dim_politique")
        
        if df_election is None:
            logger.error("‚ùå Les donn√©es √©lectorales sont None")
            return None
            
        try:
            # S√©lection et renommage des colonnes n√©cessaires
            dim_politique = df_election.select(
                col("id_parti").cast(IntegerType()).alias("etiquette_parti"),
                col("annee").cast(IntegerType()).alias("annee"),
                col("code_dept").alias("code_dept"),
                col("candidat").alias("candidat"),
                col("total_voix").cast(IntegerType()).alias("total_voix"),
                col("orientation_politique").alias("orientation_politique")
            )
            
            # Ajout d'un ID unique
            window_spec = Window.orderBy("annee", "code_dept", "candidat")
            dim_politique = dim_politique.withColumn(
                "id", F.row_number().over(window_spec)
            )
            
            # R√©organisation des colonnes pour avoir l'ID en premier
            dim_politique = dim_politique.select(
                "id", "etiquette_parti", "annee", "code_dept", "candidat", "total_voix", "orientation_politique"
            )
            
            # V√©rification que code_dept est bien limit√© √† 3 caract√®res
            dim_politique = dim_politique.withColumn(
                "code_dept",
                F.when(F.length("code_dept") > 3, F.substring("code_dept", 1, 3))
                .otherwise(F.col("code_dept"))
            )
            
            # Afficher un √©chantillon
            dim_politique.show(5, truncate=False)
            
            # R√©cup√©rer le nombre de lignes
            count_rows = dim_politique.count()
            logger.info(f"‚úÖ Dimension politique pr√©par√©e avec {count_rows} lignes")
            
            return dim_politique
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©paration des donn√©es politiques: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return None
            
    def prepare_dim_securite(self, df_securite):
        """
        Pr√©pare les donn√©es pour la dimension s√©curit√©.
        
        Args:
            df_securite: DataFrame des donn√©es de s√©curit√© (pandas)
        
        Returns:
            DataFrame format√© pour dim_securite
        """
        logger.info("üîÑ Pr√©paration des donn√©es pour dim_securite")
        
        try:
            # V√©rifier si le DataFrame est de type pandas et le convertir en DataFrame Spark
            if isinstance(df_securite, pd.DataFrame):
                logger.info("üîÑ Conversion du DataFrame pandas en DataFrame Spark")
                # Pr√©traiter c√¥t√© pandas pour √©viter probl√®mes de conversion
                df_securite = df_securite.rename(columns={
                    'Ann√©e': 'annee',
                    'D√©partement': 'code_dept',
                    'D√©lits_total': 'delits_total'
                })
                
                # S√©lection des colonnes n√©cessaires uniquement
                df_securite = df_securite[['annee', 'code_dept', 'delits_total']]
                
                # Conversion en types appropri√©s avant passage √† Spark
                df_securite['annee'] = df_securite['annee'].astype(int)
                df_securite['delits_total'] = df_securite['delits_total'].astype(int)
                df_securite['code_dept'] = df_securite['code_dept'].astype(str)
                
                # S'assurer que code_dept ne d√©passe pas 3 caract√®res
                df_securite['code_dept'] = df_securite['code_dept'].apply(
                    lambda x: x[:3] if len(x) > 3 else x
                )
                
                # Cr√©ation du sch√©ma Spark explicite
                schema = StructType([
                    StructField("annee", IntegerType(), False),
                    StructField("code_dept", StringType(), False),
                    StructField("delits_total", IntegerType(), False)
                ])
                
                # Conversion en DataFrame Spark
                spark = SparkSession.builder.getOrCreate()
                dim_securite = spark.createDataFrame(df_securite, schema=schema)
            else:
                # Si c'est d√©j√† un DataFrame Spark
                dim_securite = df_securite.select(
                    col("annee").cast(IntegerType()).alias("annee"),
                    col("code_dept").alias("code_dept"),
                    col("delits_total").cast(IntegerType()).alias("delits_total")
                )
                
            # V√©rifier que code_dept est limit√© √† 3 caract√®res
            dim_securite = dim_securite.withColumn(
                "code_dept",
                F.when(F.length("code_dept") > 3, F.substring("code_dept", 1, 3))
                .otherwise(F.col("code_dept"))
            )
            
            # L'ID sera attribu√© automatiquement par MySQL (AUTO_INCREMENT)
            
            # Afficher un √©chantillon des donn√©es (√©viter show() qui a caus√© un crash)
            logger.info(f"‚úÖ Dimension s√©curit√© pr√©par√©e avec succ√®s")
            
            return dim_securite
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©paration des donn√©es de s√©curit√©: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def prepare_dim_sante(self, df_life):
        """
        Pr√©pare les donn√©es pour la dimension sant√©.
        
        Args:
            df_life: DataFrame des donn√©es d'esp√©rance de vie
        
        Returns:
            DataFrame format√© pour dim_sante
        """
        logger.info("üîÑ Pr√©paration des donn√©es pour dim_sante")
        
        if df_life is None:
            logger.error("‚ùå Les donn√©es d'esp√©rance de vie sont None")
            return None
            
        try:
            # S√©lection et renommage des colonnes n√©cessaires
            dim_sante = df_life.select(
                col("CODE_DEP").alias("code_dept"),
                col("Ann√©e").cast(IntegerType()).alias("annee"),
                col("Esp√©rance_Vie").cast(DoubleType()).alias("esperance_vie")
            )
            
            # V√©rifier que code_dept est limit√© √† 3 caract√®res
            dim_sante = dim_sante.withColumn(
                "code_dept",
                F.when(F.length("code_dept") > 3, F.substring("code_dept", 1, 3))
                .otherwise(F.col("code_dept"))
            )
            
            # L'ID sera attribu√© automatiquement par MySQL (AUTO_INCREMENT)
            
            # Afficher un √©chantillon
            dim_sante.show(5, truncate=False)
            
            # R√©cup√©rer le nombre de lignes
            count_rows = dim_sante.count()
            logger.info(f"‚úÖ Dimension sant√© pr√©par√©e avec {count_rows} lignes")
            
            return dim_sante
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©paration des donn√©es de sant√©: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def prepare_dim_education(self, df_education):
        """
        Pr√©pare les donn√©es pour la dimension √©ducation.
        
        Args:
            df_education: DataFrame des donn√©es d'√©ducation
        
        Returns:
            DataFrame format√© pour dim_education
        """
        logger.info("üîÑ Pr√©paration des donn√©es pour dim_education")
        
        if df_education is None:
            logger.error("‚ùå Les donn√©es d'√©ducation sont None")
            return None
            
        try:
            # S√©lection et renommage des colonnes n√©cessaires
            dim_education = df_education.select(
                col("code_departement").alias("code_departement"),
                col("annee_fermeture").cast(IntegerType()).alias("annee_fermeture"),
                col("libelle_departement").alias("libelle_departement"),
                col("nombre_total_etablissements").cast(IntegerType()).alias("nombre_total_etablissements"),
                col("nb_public").cast(IntegerType()).alias("nb_public"),
                col("nb_prive").cast(IntegerType()).alias("nb_prive"),
                col("pct_public").cast(DoubleType()).alias("pct_public"),
                col("pct_prive").cast(DoubleType()).alias("pct_prive")
            )
            
            # V√©rifier que code_departement est limit√© √† 3 caract√®res
            dim_education = dim_education.withColumn(
                "code_departement",
                F.when(F.length("code_departement") > 3, F.substring("code_departement", 1, 3))
                .otherwise(F.col("code_departement"))
            )
            
            # L'ID sera attribu√© automatiquement par MySQL (AUTO_INCREMENT)
            
            # Afficher un √©chantillon
            dim_education.show(5, truncate=False)
            
            # R√©cup√©rer le nombre de lignes
            count_rows = dim_education.count()
            logger.info(f"‚úÖ Dimension √©ducation pr√©par√©e avec {count_rows} lignes")
            
            return dim_education
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©paration des donn√©es d'√©ducation: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def prepare_dim_environnement(self, df_env):
        """
        Pr√©pare les donn√©es pour la dimension environnement.
        
        Args:
            df_env: DataFrame des donn√©es environnementales
        
        Returns:
            DataFrame format√© pour dim_environnement
        """
        logger.info("üîÑ Pr√©paration des donn√©es pour dim_environnement")
        
        if df_env is None:
            logger.error("‚ùå Les donn√©es environnementales sont None")
            return None
            
        try:
            # S√©lection et renommage des colonnes n√©cessaires
            dim_environnement = df_env.select(
                col("Code_INSEE_R√©gion").alias("code_insee_region"),
                col("Ann√©e").cast(IntegerType()).alias("annee"),
                col("Parc_install√©_√©olien_MW").cast(DoubleType()).alias("parc_eolien_mw"),
                col("Parc_install√©_solaire_MW").cast(DoubleType()).alias("parc_solaire_mw")
            )
            
            # V√©rifier que code_insee_region est limit√© √† 3 caract√®res
            dim_environnement = dim_environnement.withColumn(
                "code_insee_region",
                F.when(F.length("code_insee_region") > 3, F.substring("code_insee_region", 1, 3))
                .otherwise(F.col("code_insee_region"))
            )
            
            # L'ID sera attribu√© automatiquement par MySQL (AUTO_INCREMENT)
            
            # Afficher un √©chantillon
            dim_environnement.show(5, truncate=False)
            
            # R√©cup√©rer le nombre de lignes
            count_rows = dim_environnement.count()
            logger.info(f"‚úÖ Dimension environnement pr√©par√©e avec {count_rows} lignes")
            
            return dim_environnement
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©paration des donn√©es environnementales: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def prepare_dim_socio_economie(self, df_pib_inflation):
        """
        Pr√©pare les donn√©es pour la dimension socio-√©conomie.
        
        Args:
            df_pib_inflation: DataFrame des donn√©es PIB et inflation
        
        Returns:
            DataFrame format√© pour dim_socio_economie
        """
        logger.info("üîÑ Pr√©paration des donn√©es pour dim_socio_economie")
        
        if df_pib_inflation is None:
            logger.error("‚ùå Les donn√©es PIB et inflation sont None")
            return None
            
        try:
            # S√©lection et renommage des colonnes n√©cessaires
            dim_socio_economie = df_pib_inflation.select(
                col("Ann√©e").cast(IntegerType()).alias("annee"),
                col("PIB_en_euros_par_habitant").cast(DoubleType()).alias("pib_euros_par_habitant"),
                col("Code_INSEE_R√©gion").alias("code_insee_region"),
                col("√âvolution_des_prix_√†_la_consommation").cast(DoubleType()).alias("evolution_prix_conso"),
                col("PIB_par_inflation").cast(DoubleType()).alias("pib_par_inflation")
            )
            
            # V√©rifier que code_insee_region est limit√© √† 3 caract√®res
            dim_socio_economie = dim_socio_economie.withColumn(
                "code_insee_region",
                F.when(F.length("code_insee_region") > 3, F.substring("code_insee_region", 1, 3))
                .otherwise(F.col("code_insee_region"))
            )
            
            # L'ID sera attribu√© automatiquement par MySQL (AUTO_INCREMENT)
            
            # Afficher un √©chantillon
            dim_socio_economie.show(5, truncate=False)
            
            # R√©cup√©rer le nombre de lignes
            count_rows = dim_socio_economie.count()
            logger.info(f"‚úÖ Dimension socio-√©conomie pr√©par√©e avec {count_rows} lignes")
            
            return dim_socio_economie
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©paration des donn√©es socio-√©conomiques: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def prepare_dim_technologie(self, df_tech):
        """
        Pr√©pare les donn√©es pour la dimension technologie.
        
        Args:
            df_tech: DataFrame des donn√©es technologiques
        
        Returns:
            DataFrame format√© pour dim_technologie
        """
        logger.info("üîÑ Pr√©paration des donn√©es pour dim_technologie")
        
        if df_tech is None:
            logger.error("‚ùå Les donn√©es technologiques sont None")
            return None
            
        try:
            # S√©lection et renommage des colonnes n√©cessaires
            dim_technologie = df_tech.select(
                col("annee").cast(IntegerType()).alias("annee"),
                col("dird_pib_france_pourcentages").cast(DoubleType()).alias("depenses_rd_pib")
            )
            
            # L'ID sera attribu√© automatiquement par MySQL (AUTO_INCREMENT)
            
            # Afficher un √©chantillon
            dim_technologie.show(5, truncate=False)
            
            # R√©cup√©rer le nombre de lignes
            count_rows = dim_technologie.count()
            logger.info(f"‚úÖ Dimension technologie pr√©par√©e avec {count_rows} lignes")
            
            return dim_technologie
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©paration des donn√©es technologiques: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def prepare_dim_demographie(self, df_demo):
        """
        Pr√©pare les donn√©es pour la dimension d√©mographie.
        
        Args:
            df_demo: DataFrame des donn√©es d√©mographiques
        
        Returns:
            DataFrame format√© pour dim_demographie
        """
        logger.info("üîÑ Pr√©paration des donn√©es pour dim_demographie")
        
        if df_demo is None:
            logger.error("‚ùå Les donn√©es d√©mographiques sont None")
            return None
            
        try:
            # S√©lection et renommage des colonnes n√©cessaires
            dim_demographie = df_demo.select(
                col("Ann√©e").cast(IntegerType()).alias("annee"),
                col("Code_D√©partement").alias("code_departement"),
                col("Nom_D√©partement").alias("nom_departement"),
                col("E_Total").cast(IntegerType()).alias("population_totale"),
                col("H_Total").cast(IntegerType()).alias("population_hommes"),
                col("F_Total").cast(IntegerType()).alias("population_femmes"),
                col("E_0_19_ans").cast(IntegerType()).alias("pop_0_19"),
                col("E_20_39_ans").cast(IntegerType()).alias("pop_20_39"),
                col("E_40_59_ans").cast(IntegerType()).alias("pop_40_59"),
                col("E_60_74_ans").cast(IntegerType()).alias("pop_60_74"),
                col("E_75_et_plus").cast(IntegerType()).alias("pop_75_plus")
            )
            
            # V√©rifier que code_departement est limit√© √† 3 caract√®res
            dim_demographie = dim_demographie.withColumn(
                "code_departement",
                F.when(F.length("code_departement") > 3, F.substring("code_departement", 1, 3))
                .otherwise(F.col("code_departement"))
            )
            
            # L'ID sera attribu√© automatiquement par MySQL (AUTO_INCREMENT)
            
            # Afficher un √©chantillon
            dim_demographie.show(5, truncate=False)
            
            # R√©cup√©rer le nombre de lignes
            count_rows = dim_demographie.count()
            logger.info(f"‚úÖ Dimension d√©mographie pr√©par√©e avec {count_rows} lignes")
            
            return dim_demographie
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©paration des donn√©es d√©mographiques: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def prepare_fact_resultats_politique(self, dim_politique, dim_securite, dim_socio_economie, 
                                        dim_sante, dim_environnement, dim_education, 
                                        dim_demographie, dim_technologie):
        """
        Pr√©pare la table de faits qui relie toutes les dimensions.
        
        Args:
            dim_politique: DataFrame de la dimension politique
            dim_securite: DataFrame de la dimension s√©curit√©
            dim_socio_economie: DataFrame de la dimension socio-√©conomie
            dim_sante: DataFrame de la dimension sant√©
            dim_environnement: DataFrame de la dimension environnement
            dim_education: DataFrame de la dimension √©ducation
            dim_demographie: DataFrame de la dimension d√©mographie
            dim_technologie: DataFrame de la dimension technologie
        
        Returns:
            DataFrame format√© pour fact_resultats_politique
        """
        logger.info("üîÑ Pr√©paration de la table de faits resultats_politique")
        
        try:
            # On part de la dimension politique comme base
            if dim_politique is None:
                logger.error("‚ùå La dimension politique est None, impossible de cr√©er la table de faits")
                return None
                
            # Cr√©er une cl√© unique annee_code_dpt pour la table de faits
            fact_table = dim_politique.withColumn(
                "annee_code_dpt", 
                F.concat(F.col("annee").cast("string"), F.lit("_"), F.col("code_dept"))
            )
            
            # S√©lectionner les colonnes n√©cessaires pour la table de faits
            fact_table = fact_table.select(
                "annee_code_dpt", 
                col("id").alias("id_parti"),  # id de la dimension politique
                col("annee"),
                col("code_dept")
            )
            
            # Int√©grer les ID des autres dimensions
            
            # Jointure avec dimension s√©curit√©
            if dim_securite is not None:
                logger.info("üîÑ Jointure avec dimension s√©curit√©")
                fact_table = fact_table.join(
                    dim_securite.select(
                        F.monotonically_increasing_id().alias("securite_id"), 
                        col("annee").alias("sec_annee"), 
                        col("code_dept").alias("sec_code_dept")
                    ),
                    (fact_table.annee == F.col("sec_annee")) & 
                    (fact_table.code_dept == F.col("sec_code_dept")),
                    "left"
                ).drop("sec_annee", "sec_code_dept")
            else:
                fact_table = fact_table.withColumn("securite_id", F.lit(None).cast(IntegerType()))
                
            # Jointure avec dimension socio-√©conomie (niveau r√©gion)
            if dim_socio_economie is not None:
                logger.info("üîÑ Jointure avec dimension socio-√©conomie")
                # Pour simplifier, on utilise l'ann√©e uniquement car les donn√©es sont au niveau r√©gional
                # Une approche plus pr√©cise n√©cessiterait une table de correspondance d√©partement-r√©gion
                fact_table = fact_table.join(
                    dim_socio_economie.select(
                        F.monotonically_increasing_id().alias("socio_eco_id"), 
                        col("annee").alias("eco_annee")
                    ),
                    (fact_table.annee == F.col("eco_annee")),
                    "left"
                ).drop("eco_annee")
            else:
                fact_table = fact_table.withColumn("socio_eco_id", F.lit(None).cast(IntegerType()))
                
            # Jointure avec dimension sant√©
            if dim_sante is not None:
                logger.info("üîÑ Jointure avec dimension sant√©")
                fact_table = fact_table.join(
                    dim_sante.select(
                        F.monotonically_increasing_id().alias("sante_id"), 
                        col("annee").alias("sante_annee"), 
                        col("code_dept").alias("sante_code_dept")
                    ),
                    (fact_table.annee == F.col("sante_annee")) & 
                    (fact_table.code_dept == F.col("sante_code_dept")),
                    "left"
                ).drop("sante_annee", "sante_code_dept")
            else:
                fact_table = fact_table.withColumn("sante_id", F.lit(None).cast(IntegerType()))
                
            # Jointure avec dimension environnement (niveau r√©gion)
            if dim_environnement is not None:
                logger.info("üîÑ Jointure avec dimension environnement")
                # Pour simplifier, on utilise l'ann√©e uniquement car les donn√©es sont au niveau r√©gional
                fact_table = fact_table.join(
                    dim_environnement.select(
                        F.monotonically_increasing_id().alias("environnement_id"), 
                        col("annee").alias("env_annee")
                    ),
                    (fact_table.annee == F.col("env_annee")),
                    "left"
                ).drop("env_annee")
            else:
                fact_table = fact_table.withColumn("environnement_id", F.lit(None).cast(IntegerType()))
                
            # Jointure avec dimension √©ducation
            if dim_education is not None:
                logger.info("üîÑ Jointure avec dimension √©ducation")
                fact_table = fact_table.join(
                    dim_education.select(
                        F.monotonically_increasing_id().alias("education_id"), 
                        col("annee_fermeture").alias("edu_annee"), 
                        col("code_departement").alias("edu_code_dept")
                    ),
                    (fact_table.annee == F.col("edu_annee")) & 
                    (fact_table.code_dept == F.col("edu_code_dept")),
                    "left"
                ).drop("edu_annee", "edu_code_dept")
            else:
                fact_table = fact_table.withColumn("education_id", F.lit(None).cast(IntegerType()))
                
            # Jointure avec dimension d√©mographie
            if dim_demographie is not None:
                logger.info("üîÑ Jointure avec dimension d√©mographie")
                fact_table = fact_table.join(
                    dim_demographie.select(
                        F.monotonically_increasing_id().alias("demographie_id"), 
                        col("annee").alias("demo_annee"), 
                        col("code_departement").alias("demo_code_dept")
                    ),
                    (fact_table.annee == F.col("demo_annee")) & 
                    (fact_table.code_dept == F.col("demo_code_dept")),
                    "left"
                ).drop("demo_annee", "demo_code_dept")
            else:
                fact_table = fact_table.withColumn("demographie_id", F.lit(None).cast(IntegerType()))
                
            # Jointure avec dimension technologie (niveau national)
            if dim_technologie is not None:
                logger.info("üîÑ Jointure avec dimension technologie")
                fact_table = fact_table.join(
                    dim_technologie.select(
                        F.monotonically_increasing_id().alias("technologie_id"), 
                        col("annee").alias("tech_annee")
                    ),
                    (fact_table.annee == F.col("tech_annee")),
                    "left"
                ).drop("tech_annee")
            else:
                fact_table = fact_table.withColumn("technologie_id", F.lit(None).cast(IntegerType()))
            
            # S√©lectionner les colonnes finales pour la table de faits
            fact_table = fact_table.select(
                "annee_code_dpt",
                "id_parti",
                "securite_id",
                "socio_eco_id",
                "sante_id",
                "environnement_id",
                "education_id",
                "demographie_id",
                "technologie_id"
            )
            
            # IMPORTANT: Nous utilisons count() au lieu de show() pour √©viter les probl√®mes d'affichage
            count_rows = fact_table.count()
            logger.info(f"‚úÖ Table de faits pr√©par√©e avec {count_rows} lignes")
            
            return fact_table
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la pr√©paration de la table de faits: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    # TODO Rename this here and in `transform_environmental_data`, `transform_pib_outre_mer`, `fill_missing_pib_mayotte`, `combine_all_pib_data`, `transform_inflation_data`, `combine_pib_and_inflation`, `transform_education_data`, `calculate_closed_by_year_and_dept_education`, `transform_demography_data` and `combine_election_and_orientation_politique`
    def _extracted_from_combine_election_and_orientation_politique_52(
        self, arg0, arg1, arg2
    ):
        return self._extracted_from__extracted_from_combine_election_and_orientation_politique_52_116(
            arg0, arg1, arg2
        )

    # TODO Rename this here and in `calculate_closed_by_year_and_dept_education`, `transform_demography_data` and `_extracted_from_combine_election_and_orientation_politique_52`
    def _extracted_from__extracted_from_combine_election_and_orientation_politique_52_116(self, arg0, arg1, arg2):
        logger.info(arg0)
        arg1.show(arg2, truncate=False)
        return arg1

    # TODO Rename this here and in `calculate_closed_by_year_and_dept_education`, `transform_demography_data` and `_extracted_from_combine_election_and_orientation_politique_52`
    def _extracted_from__extracted_from_combine_election_and_orientation_politique_52_116(self, arg0, arg1, arg2):
        logger.info(arg0)
        arg1.show(arg2, truncate=False)
        return arg1
