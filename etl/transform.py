# transform.py

import logging
from pyspark.sql.functions import (
    col,
    when,
    lit,
    sum as spark_sum,
    round,
    regexp_replace,
    regexp_extract,
    expr,
    trim,
    upper,
    create_map,
    coalesce,
)
from pyspark.sql.types import IntegerType, DoubleType, DateType
from pyspark.sql.window import Window
from pyspark.sql import functions as F, types as T
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler
from itertools import chain

# Configuration du logger
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DataTransformer:
    """
    Classe permettant de transformer les donnÃ©es extraites avant leur chargement.
    """

    def __init__(self):
        logger.info("ðŸš€ Initialisation du DataTransformer")

    def transform_environmental_data(self, df_env):
        """
        Transforme les donnÃ©es environnementales :
        - SÃ©lectionne uniquement les colonnes nÃ©cessaires
        - Remplace les valeurs vides par 0.0 pour les valeurs manquantes d'Ã©olien et solaire
        - Ajoute les donnÃ©es de l'annÃ©e 2000 pour chaque rÃ©gion avec des valeurs Ã  0.0
        - Regroupe les donnÃ©es par Code_INSEE_rÃ©gion et AnnÃ©e (somme en cas de doublons)
        - Trie les rÃ©sultats par rÃ©gion et annÃ©e
        """

        if df_env is None:
            logger.error("âŒ Le DataFrame environnemental est vide ou invalide.")
            return None

        logger.info("ðŸš€ Transformation des donnÃ©es environnementales en cours...")

        # SÃ©lection des colonnes nÃ©cessaires et cast des valeurs
        df_transformed = df_env.select(
            col("AnnÃ©e").cast("int"),
            col("Code_INSEE_RÃ©gion"),
            col("Parc_installÃ©_Ã©olien_MW").cast("double"),
            col("Parc_installÃ©_solaire_MW").cast("double"),
        )

        # Remplacement des valeurs nulles par 0.0
        df_transformed = df_transformed.fillna(
            {"Parc_installÃ©_Ã©olien_MW": 0.0, "Parc_installÃ©_solaire_MW": 0.0}
        )

        # Regroupement par rÃ©gion et annÃ©e pour sommer les valeurs en cas de doublons
        df_grouped = df_transformed.groupBy("Code_INSEE_RÃ©gion", "AnnÃ©e").agg(
            spark_sum("Parc_installÃ©_Ã©olien_MW").alias("Parc_installÃ©_Ã©olien_MW"),
            spark_sum("Parc_installÃ©_solaire_MW").alias("Parc_installÃ©_solaire_MW"),
        )

        # RÃ©cupÃ©ration des rÃ©gions uniques prÃ©sentes dans les donnÃ©es
        regions = df_grouped.select("Code_INSEE_RÃ©gion").distinct()

        # CrÃ©ation d'un DataFrame contenant l'annÃ©e 2000 pour chaque rÃ©gion avec valeurs Ã  0.0
        df_year_2000 = (
            regions.withColumn("AnnÃ©e", lit(2000))
            .withColumn("Parc_installÃ©_Ã©olien_MW", lit(0.0))
            .withColumn("Parc_installÃ©_solaire_MW", lit(0.0))
        )

        # Ajout des donnÃ©es de l'annÃ©e 2000 au DataFrame principal
        df_final = df_grouped.union(df_year_2000)

        # Tri des donnÃ©es par rÃ©gion et annÃ©e
        df_final = df_final.orderBy("Code_INSEE_RÃ©gion", "AnnÃ©e")

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "âœ… Transformation terminÃ©e ! AperÃ§u des donnÃ©es transformÃ©es :",
            df_final,
            15,
        )

    def transform_pib_outre_mer(self, df_pib, region_codes):
        """
        Transforme les donnÃ©es PIB outre-mer :
        - Suppression des lignes inutiles
        - Ajout du code rÃ©gion INSEE Ã  partir du nom du fichier
        - Tri par RÃ©gion puis AnnÃ©e
        """

        if df_pib is None:
            logger.error("âŒ Le DataFrame PIB est vide ou invalide.")
            return None

        logger.info("ðŸš€ Transformation des donnÃ©es PIB outre-mer en cours...")

        # Nettoyage des donnÃ©es
        df_cleaned = df_pib.filter(
            (~col("AnnÃ©e").isin(["idBank", "DerniÃ¨re mise Ã  jour", "PÃ©riode"]))
            & (col("AnnÃ©e").rlike("^[0-9]{4}$"))
        ).select(
            col("AnnÃ©e").cast("int"),
            col("PIB_en_euros_par_habitant").cast("int"),
            col("source_file"),
        )

        # Ajout du code rÃ©gion INSEE depuis le dictionnaire region_codes
        condition = None
        for file_path, code_region in region_codes.items():
            if condition is None:
                condition = when(col("source_file") == file_path, lit(code_region))
            else:
                condition = condition.when(
                    col("source_file") == file_path, lit(code_region)
                )

        df_final = df_cleaned.withColumn("Code_INSEE_RÃ©gion", lit(None))
        for file_path, code_region in region_codes.items():
            df_final = df_final.withColumn(
                "Code_INSEE_RÃ©gion",
                when(col("source_file") == file_path, lit(code_region)).otherwise(
                    col("Code_INSEE_RÃ©gion")
                ),
            )

        df_final = df_final.drop("source_file")

        # Tri final
        df_final = df_final.orderBy(["Code_INSEE_RÃ©gion", "AnnÃ©e"])

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "âœ… Transformation PIB terminÃ©e ! AperÃ§u des donnÃ©es transformÃ©es :",
            df_final,
            10,
        )

    def fill_missing_pib_mayotte(self, df_pib):
        """
        Remplit les valeurs manquantes du PIB de Mayotte par rÃ©gression linÃ©aire.
        """

        logger.info("ðŸš€ Remplissage des valeurs manquantes PIB Mayotte en cours...")

        df_mayotte = df_pib.filter(col("Code_INSEE_RÃ©gion") == "06")

        known_data = df_mayotte.filter(col("PIB_en_euros_par_habitant").isNotNull())
        unknown_data = df_mayotte.filter(col("PIB_en_euros_par_habitant").isNull())

        assembler = VectorAssembler(inputCols=["AnnÃ©e"], outputCol="features")
        train_data = assembler.transform(known_data).select(
            "features", "PIB_en_euros_par_habitant"
        )

        # ModÃ¨le de rÃ©gression linÃ©aire
        lr = LinearRegression(
            featuresCol="features", labelCol="PIB_en_euros_par_habitant"
        )
        model = lr.fit(train_data)

        # PrÃ©dictions sur les donnÃ©es manquantes
        pred_df = assembler.transform(unknown_data)
        pred_result = model.transform(pred_df).select(
            "AnnÃ©e",
            col("prediction").cast("int").alias("PIB_en_euros_par_habitant"),
            "Code_INSEE_RÃ©gion",
        )

        # Combine les donnÃ©es connues et prÃ©dites
        df_mayotte_completed = known_data.select(
            "AnnÃ©e", "PIB_en_euros_par_habitant", "Code_INSEE_RÃ©gion"
        ).union(pred_result)

        # Autres rÃ©gions sans modifications
        df_other_regions = df_pib.filter(col("Code_INSEE_RÃ©gion") != "06")

        # Union finale
        df_final = df_other_regions.union(df_mayotte_completed).orderBy(
            ["Code_INSEE_RÃ©gion", "AnnÃ©e"]
        )

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "âœ… Remplissage PIB Mayotte terminÃ© :", df_final, 10
        )

    def combine_all_pib_data(self, df_pib_outremer, df_pib_xlsx, df_pib_2022):
        """
        Combine les donnÃ©es PIB des diffÃ©rentes sources en un seul DataFrame.
        """

        logger.info("ðŸš€ Fusion des donnÃ©es PIB (Outre-mer, Excel, 2022)...")

        # Harmoniser les colonnes
        df_pib_xlsx = df_pib_xlsx.select(
            "AnnÃ©e", "PIB_en_euros_par_habitant", "Code_INSEE_RÃ©gion"
        )
        df_pib_2022 = df_pib_2022.select(
            "AnnÃ©e", "PIB_en_euros_par_habitant", "Code_INSEE_RÃ©gion"
        )
        df_pib_outremer = df_pib_outremer.select(
            "AnnÃ©e", "PIB_en_euros_par_habitant", "Code_INSEE_RÃ©gion"
        )

        # Liste des rÃ©gions prÃ©sentes en 2022
        regions_2022 = [
            row["Code_INSEE_RÃ©gion"]
            for row in df_pib_2022.select("Code_INSEE_RÃ©gion").distinct().collect()
        ]

        # Identifier les rÃ©gions absentes en 2022
        missing_regions = (
            df_pib_xlsx.select("Code_INSEE_RÃ©gion")
            .distinct()
            .filter(~col("Code_INSEE_RÃ©gion").isin(regions_2022))
        )

        # Ajouter des lignes vides pour les rÃ©gions absentes en 2022
        if missing_regions.count() > 0:
            df_missing_2022 = missing_regions.withColumn("AnnÃ©e", lit(2022)).withColumn(
                "PIB_en_euros_par_habitant", lit(None).cast("int")
            )
            df_pib_2022 = df_pib_2022.union(df_missing_2022)

        # Fusion des donnÃ©es
        df_final = df_pib_outremer.union(df_pib_xlsx).union(df_pib_2022)

        # **Filtrer les lignes invalides** (Code rÃ©gion doit Ãªtre numÃ©rique et PIB non NULL)
        df_final = df_final.filter(
            (col("Code_INSEE_RÃ©gion").rlike("^[0-9]+$"))
            & (col("PIB_en_euros_par_habitant").isNotNull())
        )

        # Filtrer et trier
        df_final = df_final.filter((col("AnnÃ©e") >= 2000) & (col("AnnÃ©e") <= 2022))
        df_final = df_final.orderBy(["Code_INSEE_RÃ©gion", "AnnÃ©e"])

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "âœ… Fusion des donnÃ©es PIB rÃ©ussie :", df_final, 10
        )

    def transform_inflation_data(self, df_inflation):
        """
        Transforme les donnÃ©es d'inflation en filtrant les annÃ©es et en les triant.

        :param df_inflation: DataFrame PySpark contenant les donnÃ©es brutes d'inflation.
        :return: DataFrame PySpark nettoyÃ© et triÃ©.
        """
        if df_inflation is None:
            logger.error("âŒ Le DataFrame inflation est vide ou invalide.")
            return None

        logger.info("ðŸš€ Transformation des donnÃ©es d'inflation en cours...")

        # Filtrer et trier les donnÃ©es
        df_transformed = df_inflation.orderBy("AnnÃ©e")

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "âœ… Transformation des donnÃ©es d'inflation rÃ©ussie :",
            df_transformed,
            10,
        )

    def combine_pib_and_inflation(self, df_pib, df_inflation):
        """
        Combine les donnÃ©es PIB et Inflation, et calcule le ratio PIB_par_inflation avec arrondi Ã  2 dÃ©cimales.

        :param df_pib: DataFrame PySpark contenant le PIB par rÃ©gion.
        :param df_inflation: DataFrame PySpark contenant l'inflation nationale.
        :return: DataFrame PySpark combinÃ© avec le calcul du PIB ajustÃ© par l'inflation.
        """
        if df_pib is None or df_inflation is None:
            logger.error("âŒ L'un des DataFrames est vide. Impossible de les combiner.")
            return None

        logger.info("ðŸš€ Fusion des donnÃ©es PIB et Inflation...")

        # Joindre PIB et Inflation sur la colonne AnnÃ©e
        df_combined = df_pib.join(df_inflation, "AnnÃ©e", "left")

        # Utiliser le bon nom de colonne pour l'inflation et arrondir Ã  2 dÃ©cimales
        df_combined = df_combined.withColumn(
            "Ã‰volution_des_prix_Ã _la_consommation",
            round(col("Ã‰volution_des_prix_Ã _la_consommation"), 2),
        )

        df_combined = df_combined.withColumn(
            "PIB_par_inflation",
            round(
                col("PIB_en_euros_par_habitant")
                / (1 + col("Ã‰volution_des_prix_Ã _la_consommation") / 100),
                2,
            ),
        )

        # Trier les rÃ©sultats
        df_combined = df_combined.orderBy("Code_INSEE_RÃ©gion", "AnnÃ©e")

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "âœ… Fusion des donnÃ©es PIB et Inflation rÃ©ussie :", df_combined, 10
        )

    def transform_technologie_data(self, df):
        """
        Transforme les donnÃ©es de technologie.

        :param df: DataFrame PySpark brut
        :return: DataFrame PySpark transformÃ©
        """
        if df is None:
            logger.error("âŒ Le DataFrame technologie est vide ou invalide.")
            return None

        logger.info("ðŸš€ Transformation des donnÃ©es de technologie en cours...")

        try:
            return self._extracted_from_transform_technologie_data_15(df)
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la transformation des donnÃ©es : {str(e)}")
            return None

    # TODO Rename this here and in `transform_technologie_data`
    def _extracted_from_transform_technologie_data_15(self, df):
        df_transformed = self._select_and_rename_columns(df)
        df_transformed = self._round_percentages(df_transformed)
        df_transformed = self._clean_years(df_transformed)
        df_transformed = self._replace_nan_year(df_transformed)

        logger.info("âœ… Transformation des donnÃ©es de technologie rÃ©ussie")
        return df_transformed

    def _select_and_rename_columns(self, df):
        """SÃ©lectionne et renomme les colonnes."""
        return df.select(
            col("_c0").alias("annee").cast("string"),
            col("DIRD/PIB  France").alias("dird_pib_france_pourcentages").cast("float"),
        )

    def _round_percentages(self, df):
        """Arrondit les pourcentages Ã  2 dÃ©cimales."""
        return df.withColumn(
            "dird_pib_france_pourcentages",
            round(col("dird_pib_france_pourcentages"), 2),
        )

    def _clean_years(self, df):
        """Nettoie les annÃ©es en supprimant '.0'."""
        return df.withColumn("annee", regexp_replace("annee", "\.0", ""))

    def _replace_nan_year(self, df):
        """Remplace les valeurs 'NaN' dans la colonne 'annee' par '2023'."""
        return df.withColumn(
            "annee", when(col("annee") == "NaN", "2023").otherwise(col("annee"))
        )

    def transform_election_data_1965_2012(self, list_df):
        """
        Transforme et agrÃ¨ge les fichiers CSV 1965-2012.
        - Unpivot via STACK
        - AgrÃ©gation par candidat
        - SÃ©lection du candidat gagnant par dÃ©partement et annÃ©e
        - Nettoyage du nom du candidat
        :param list_df: liste de DataFrames bruts
        :return: DataFrame PySpark final (annÃ©e, code_dept, candidat, total_voix)
        """
        if not list_df:
            logger.warning("Liste de DataFrames 1965-2012 vide.")
            return None
        results = []
        # Boucle sur chacun des DF (un par fichier CSV)
        for df in list_df:
            # Colonnes clÃ©s (Ã  ne pas unpivoter)
            key_columns = {
                "Code dÃ©partement",
                "Code dÃ©partement0",
                "Code dÃ©partement1",
                "dÃ©partement",
                "circonscription",
                "Inscrits",
                "Votants",
                "ExprimÃ©s",
                "Blancs et nuls",
                "filename",
                "annee",
            }

            # DÃ©termine la bonne colonne de dÃ©partement
            if "Code dÃ©partement" in df.columns:
                dept_col = "Code dÃ©partement"
            elif "Code dÃ©partement0" in df.columns:
                dept_col = "Code dÃ©partement0"
            else:
                # Pas de colonne attendue
                continue

            # Colonnes candidats
            candidate_columns = [c for c in df.columns if c not in key_columns]
            n = len(candidate_columns)
            if n == 0:
                continue

            # Expression stack pour unpivot
            expr_parts = []
            for c in candidate_columns:
                escaped_col = c.replace("'", "''")
                expr_parts.append(f"'{escaped_col}', cast(`{c}` as int)")
            expr = f"stack({n}, {', '.join(expr_parts)}) as (candidat, voix)"

            # Unpivot
            df_unpivot = df.select(
                "annee", F.col(dept_col).alias("code_dept"), F.expr(expr)
            )

            # AgrÃ©gation par dÃ©partement / candidat / annÃ©e
            df_agg = df_unpivot.groupBy("annee", "code_dept", "candidat").agg(
                F.sum("voix").alias("total_voix")
            )

            # SÃ©lection du gagnant par dept + annÃ©e
            windowSpec = Window.partitionBy("annee", "code_dept").orderBy(
                F.desc("total_voix")
            )
            df_winner = df_agg.withColumn(
                "rank", F.row_number().over(windowSpec)
            ).filter(F.col("rank") == 1)

            # Nettoyage du nom du candidat
            df_winner = df_winner.withColumn("gagnant", F.col("candidat"))
            df_winner = df_winner.withColumn(
                "gagnant", F.trim(F.regexp_replace(F.col("gagnant"), r"\([^)]*\)", ""))
            )
            df_winner = df_winner.withColumn(
                "gagnant",
                when(F.col("gagnant") == "SARKOZY", "Nicolas SARKOZY")
                .when(F.col("gagnant") == "CHIRAC", "Jacques CHIRAC")
                .when(F.col("gagnant") == "MITTERRAND", "FranÃ§ois MITTERRAND")
                .when(F.col("gagnant") == "DE GAULLE", "Charles DE GAULLE")
                .when(F.col("gagnant") == "GISCARD DESTAING", "ValÃ©ry GISCARD DESTAING")
                .when(F.col("gagnant") == "POMPIDOU", "Georges POMPIDOU")
                .when(F.col("gagnant") == "POHER", "Alain POHER")
                .when(F.col("gagnant") == "JOSPIN", "Lionel JOSPIN")
                .when(F.col("gagnant") == "ROYAL", "SÃ©golÃ¨ne ROYAL")
                .when(F.col("gagnant") == "HOLLANDE", "FranÃ§ois HOLLANDE")
                .when(F.col("gagnant") == "MACRON", "Emmanuel MACRON")
                .when(F.col("gagnant") == "LE PEN", "Marine LE PEN")
                .otherwise(F.col("gagnant")),
            )

            # SÃ©lection colonnes finales
            results.append(
                df_winner.select(
                    "annee",
                    "code_dept",
                    F.col("gagnant").alias("candidat"),
                    "total_voix",
                )
            )

        # Union de tous les rÃ©sultats
        if results:
            final_df = results[0]
            for df_r in results[1:]:
                final_df = final_df.union(df_r)

            # Normalisation du code_dept (ex: passer '1' -> '01')
            final_df = (
                final_df.withColumn(
                    "code_dept_norm",
                    F.when(
                        F.col("code_dept").rlike("^[0-9]$"),
                        F.lpad(F.col("code_dept"), 2, "0"),
                    ).otherwise(F.col("code_dept")),
                )
                .drop("code_dept")
                .withColumnRenamed("code_dept_norm", "code_dept")
            )

            return final_df

        else:
            logger.warning("Aucune donnÃ©e agrÃ©gÃ©e pour 1965-2012.")
            return None

    def transform_election_data_2017(self, df_2017_raw):
        """
        Transforme le fichier Excel 2017 :
        - SÃ©lection du candidat gagnant par dÃ©partement
        - Nettoyage (codes spÃ©ciaux pour rÃ©gions d outre-mer)
        """
        if df_2017_raw is None:
            logger.warning("DataFrame 2017 vide.")
            return None

        df_2017 = (
            df_2017_raw.withColumnRenamed("Code du dÃ©partement", "code_dept")
            .withColumn(
                "candidat1", F.concat(F.col("Nom17"), F.lit(" "), F.col("PrÃ©nom18"))
            )
            .withColumn(
                "candidat2", F.concat(F.col("Nom23"), F.lit(" "), F.col("PrÃ©nom24"))
            )
            .select(
                F.col("code_dept").cast("string"),
                F.col("LibellÃ© du dÃ©partement"),
                F.col("Voix19").alias("voix1").cast("int"),
                F.col("Voix25").alias("voix2").cast("int"),
                "candidat1",
                "candidat2",
            )
        )

        # On crÃ©e un DataFrame par candidat
        df_2017_candidate1 = df_2017.select(
            "code_dept",
            F.col("candidat1").alias("candidat"),
            F.col("voix1").alias("voix"),
            F.col("LibellÃ© du dÃ©partement"),
        )

        df_2017_candidate2 = df_2017.select(
            "code_dept",
            F.col("candidat2").alias("candidat"),
            F.col("voix2").alias("voix"),
            F.col("LibellÃ© du dÃ©partement"),
        )

        # Union des deux candidats
        df_2017_norm = df_2017_candidate1.union(df_2017_candidate2).withColumn(
            "annee", F.lit("2017")
        )

        # 1. Appliquer le mapping pour les codes spÃ©ciaux et la Corse
        df_2017_norm = df_2017_norm.withColumn(
            "code_dept_norm",
            F.when(F.col("LibellÃ© du dÃ©partement") == "Guadeloupe", "ZA")
            .when(F.col("LibellÃ© du dÃ©partement") == "Martinique", "ZB")
            .when(F.col("LibellÃ© du dÃ©partement") == "Guyane", "ZC")
            .when(F.col("LibellÃ© du dÃ©partement") == "La RÃ©union", "ZD")
            .when(F.col("LibellÃ© du dÃ©partement") == "Mayotte", "ZM")
            .when(F.col("LibellÃ© du dÃ©partement") == "Nouvelle-CalÃ©donie", "ZN")
            .when(F.col("LibellÃ© du dÃ©partement") == "PolynÃ©sie franÃ§aise", "ZP")
            .when(F.col("LibellÃ© du dÃ©partement") == "Saint-Pierre-et-Miquelon", "ZS")
            .when(
                F.col("LibellÃ© du dÃ©partement") == "Saint-Martin/Saint-BarthÃ©lemy", "ZX"
            )
            .when(F.col("LibellÃ© du dÃ©partement") == "Wallis et Futuna", "ZW")
            .when(
                F.col("LibellÃ© du dÃ©partement") == "FranÃ§ais Ã©tablis hors de France",
                "ZZ",
            )
            .when(F.col("LibellÃ© du dÃ©partement") == "Corse-du-Sud", "2A")
            .when(F.col("LibellÃ© du dÃ©partement") == "Haute-Corse", "2B")
            .otherwise(F.col("code_dept")),
        )

        # 1. Supprimer la terminaison ".0" dans la colonne "code_dept_norm"
        df_final_2017 = df_2017_norm.withColumn(
            "code_dept_final", F.regexp_replace(F.col("code_dept_norm"), r"\.0$", "")
        )

        # 2. (Optionnel) Si vous souhaitez que les codes sur un seul chiffre soient affichÃ©s sur 2 chiffres (ex. "1" -> "01")
        df_final_2017 = df_final_2017.withColumn(
            "code_dept_final",
            F.when(
                F.col("code_dept_final").rlike("^[0-9]$"),
                F.lpad(F.col("code_dept_final"), 2, "0"),
            ).otherwise(F.col("code_dept_final")),
        )

        # 3. Supprimer les colonnes intermÃ©diaires et renommer la colonne finale en "code_dept"
        df_final_2017 = df_final_2017.drop(
            "code_dept", "code_dept_norm", "LibellÃ© du dÃ©partement"
        ).withColumnRenamed("code_dept_final", "code_dept")

        # Pour chaque dÃ©partement, on garde le candidat avec le maximum de voix
        w_dept = Window.partitionBy("annee", "code_dept").orderBy(F.desc("voix"))
        return (
            df_final_2017.withColumn("rank", F.row_number().over(w_dept))
            .filter(F.col("rank") == 1)
            .select("annee", "code_dept", "candidat", "voix")
        )

    def transform_election_data_2022(self, df_2022_raw):
        """
        Transforme le fichier Excel 2022 :
        - SÃ©lection du gagnant par dÃ©partement
        - Normalisation du nom du candidat (Emmanuel MACRON, Marine LE PEN, etc.)
        """
        if df_2022_raw is None:
            logger.warning("DataFrame 2022 vide.")
            return None

        # Pour 2022, on suppose que chaque ligne correspond dÃ©jÃ  Ã  un candidat,
        # avec "Code du dÃ©partement", "Nom", "PrÃ©nom" et "Voix".
        df_2022 = (
            df_2022_raw.withColumnRenamed("Code du dÃ©partement", "code_dept")
            .withColumn("candidat", F.concat(F.col("Nom"), F.lit(" "), F.col("PrÃ©nom")))
            .select(
                F.col("code_dept").cast("string"),
                "candidat",
                F.col("Voix").alias("voix"),
            )
            .withColumn("annee", F.lit("2022"))
        )

        # On agrÃ¨ge par dÃ©partement pour sÃ©lectionner le candidat gagnant (le plus de voix)
        w_dept_2022 = Window.partitionBy("annee", "code_dept").orderBy(F.desc("voix"))
        return (
            df_2022.withColumn("rank", F.row_number().over(w_dept_2022))
            .filter(F.col("rank") == 1)
            .select("annee", "code_dept", "candidat", "voix")
        )

    def combine_all_years(self, df_1965_2012, df_2017, df_2022):
        """
        Combine les DataFrames de 1965-2012, 2017 et 2022.
        Applique les mappings DOM-TOM (ZA->971 etc.) et renomme la colonne 'voix' en 'total_voix'.
        """
        if df_1965_2012 is None and df_2017 is None and df_2022 is None:
            logger.warning("Aucun DataFrame Ã  combiner.")
            return None

        # Union 2017 et 2022
        df_final = df_2017.union(df_2022)

        # 1. Appliquer le mapping pour les codes spÃ©ciaux
        df_final = df_final.withColumn(
            "code_dept",
            F.when(F.col("code_dept") == "ZA", "971")
            .when(F.col("code_dept") == "ZB", "972")
            .when(F.col("code_dept") == "ZC", "973")
            .when(F.col("code_dept") == "ZD", "974")
            .when(F.col("code_dept") == "ZM", "976")
            .when(F.col("code_dept") == "ZN", "988")
            .when(F.col("code_dept") == "ZP", "987")
            .when(F.col("code_dept") == "ZS", "975")
            .when(F.col("code_dept") == "ZX", "971")
            .when(F.col("code_dept") == "ZW", "986")
            .when(F.col("code_dept") == "ZZ", "99")
            .otherwise(F.col("code_dept")),
        )

        # 1.1 Normalisation des candidats avec prÃ©nom et nom
        df_final = df_final.withColumn(
            "candidat",
            F.when(F.col("candidat") == "MACRON Emmanuel", "Emmanuel MACRON").when(
                F.col("candidat") == "LE PEN Marine", "Marine LE PEN"
            ),
        )

        # 2. Appliquer le format int pour les voix
        df_final = df_final.withColumn("voix", F.col("voix").cast("int"))

        # 3. Renommer la colonne "voix" en "total_voix"
        df_final = df_final.withColumnRenamed("voix", "total_voix")

        # 4. SÃ©lection des colonnes d'intÃ©rÃªt
        df_1965_2012 = df_1965_2012.select(
            "annee", "code_dept", "candidat", "total_voix"
        )

        # 5. Union des deux DataFrames
        df_final_csv = df_final.union(df_1965_2012)

        # Tri final
        df_final_csv = df_final_csv.orderBy("annee", "code_dept")

        return df_final_csv

    def transform_life_expectancy_data(self, df_life, df_departments):
        """
        Transforme les donnÃ©es d'espÃ©rance de vie Ã  la naissance pour hommes et femmes pour
        obtenir une colonne unique "EspÃ©rance_Vie" correspondant Ã  la moyenne des valeurs hommes et femmes.

        Ã‰tapes :
        - Filtrage des lignes dont le libellÃ© commence par "EspÃ©rance de vie Ã  la naissance - Hommes" ou "EspÃ©rance de vie Ã  la naissance - Femmes"
        - Extraction du genre et du nom de dÃ©partement ou rÃ©gion depuis le libellÃ©
        - SÃ©lection des colonnes des annÃ©es (2000 Ã  2022) et conversion du format large en format long via STACK
        - Pivot sur la colonne "Genre" pour obtenir deux colonnes ("Hommes" et "Femmes")
        - Normalisation des noms pour ne conserver que les dÃ©partements rÃ©els (Ã  l'aide du DataFrame des dÃ©partements)
        - Jointure avec le DataFrame des dÃ©partements pour rÃ©cupÃ©rer le code dÃ©partement rÃ©el (CODE_DEP)
        - Calcul de la moyenne de "Hommes" et "Femmes" et crÃ©ation d'une colonne unique "EspÃ©rance_Vie"

        :param df_life: DataFrame PySpark contenant les donnÃ©es brutes d'espÃ©rance de vie.
        :param df_departments: DataFrame PySpark contenant les donnÃ©es des dÃ©partements.
        :return: DataFrame final avec colonnes CODE_DEP, AnnÃ©e, EspÃ©rance_Vie.
        """
        if df_life is None:
            logger.error("âŒ Le DataFrame d'espÃ©rance de vie est vide ou invalide.")
            return None

        logger.info("ðŸš€ Transformation des donnÃ©es d'espÃ©rance de vie en cours...")

        # Filtrer les lignes d'intÃ©rÃªt
        df_filtered = df_life.filter(
            (col("LibellÃ©").rlike("^EspÃ©rance de vie Ã  la naissance - Hommes"))
            | (col("LibellÃ©").rlike("^EspÃ©rance de vie Ã  la naissance - Femmes"))
        )

        # Extraire le genre et le "nom de dÃ©partement ou rÃ©gion" depuis le libellÃ©
        df_filtered = df_filtered.withColumn(
            "Genre",
            regexp_extract(
                col("LibellÃ©"),
                r"EspÃ©rance de vie Ã  la naissance - (Hommes|Femmes) - (.*)",
                1,
            ),
        ).withColumn(
            "DÃ©partement",
            trim(
                regexp_extract(
                    col("LibellÃ©"),
                    r"EspÃ©rance de vie Ã  la naissance - (Hommes|Femmes) - (.*)",
                    2,
                )
            ),
        )

        # SÃ©lectionner les colonnes des annÃ©es de 2000 Ã  2022
        years = [str(year) for year in range(2000, 2023)]
        selected_cols = ["LibellÃ©", "Genre", "DÃ©partement"] + years
        df_selected = df_filtered.select(*selected_cols)

        # Conversion du format large en format long via STACK
        n_years = len(years)
        stack_expr = "stack({0}, {1}) as (Annee, Esperance_de_vie)".format(
            n_years, ", ".join([f"'{year}', `{year}`" for year in years])
        )
        df_long = df_selected.select("Genre", "DÃ©partement", expr(stack_expr))
        df_long = df_long.withColumn(
            "Annee", col("Annee").cast(IntegerType())
        ).withColumn("Esperance_de_vie", col("Esperance_de_vie").cast(DoubleType()))
        df_long = df_long.filter(col("Annee").between(2000, 2022))

        # Pivot pour crÃ©er des colonnes pour Hommes et Femmes
        df_pivot = (
            df_long.groupBy("DÃ©partement", "Annee")
            .pivot("Genre", ["Hommes", "Femmes"])
            .agg(F.first("Esperance_de_vie"))
        )

        # Fonction de normalisation des noms
        def normalize_dept(column):
            norm = F.lower(trim(column))
            # Remplacer les accents par leurs Ã©quivalents non accentuÃ©s
            norm = F.translate(norm, "Ã©Ã¨ÃªÃ«Ã Ã¢Ã¤Ã®Ã¯Ã´Ã¶Ã¹Ã»Ã¼Ã§", "eeeeaaaiioouuuc")
            # Supprimer tirets, apostrophes et espaces
            norm = F.regexp_replace(norm, "[-' ]", "")
            return norm

        # Appliquer la normalisation sur le DataFrame pivotÃ©
        df_pivot = df_pivot.withColumn(
            "DÃ©partement_norm", normalize_dept(col("DÃ©partement"))
        )
        # Normaliser le DataFrame des dÃ©partements
        df_depts_norm = df_departments.withColumn(
            "nom_departement_norm", normalize_dept(col("nom_departement"))
        )

        # Filtrage : ne conserver que les lignes correspondant Ã  des dÃ©partements rÃ©els
        valid_dept_names = [
            row["nom_departement_norm"]
            for row in df_depts_norm.select("nom_departement_norm").distinct().collect()
        ]
        logger.info(
            "Liste des dÃ©partements valides (normalisÃ©s) : "
            + ", ".join(valid_dept_names)
        )
        df_pivot = df_pivot.filter(col("DÃ©partement_norm").isin(valid_dept_names))

        # Jointure pour associer le code de dÃ©partement rÃ©el
        df_joined = df_pivot.join(
            df_depts_norm,
            df_pivot["DÃ©partement_norm"] == df_depts_norm["nom_departement_norm"],
            "left",
        )

        # Calcul de la moyenne des deux colonnes pour obtenir une seule colonne "EspÃ©rance_Vie"
        df_final = df_joined.select(
            df_depts_norm["code_departement"].alias("CODE_DEP"),
            col("Annee").alias("AnnÃ©e"),
            round(((col("Hommes") + col("Femmes")) / 2), 2).alias("EspÃ©rance_Vie"),
        ).orderBy("CODE_DEP", "AnnÃ©e")

        logger.info("âœ… Transformation terminÃ©e !")

        return df_final

    def fill_missing_mayotte_life_expectancy(self, df_final):
        """
        ComplÃ¨te les valeurs manquantes pour Mayotte (CODE_DEP = "976")
        dans le DataFrame final en utilisant une rÃ©gression linÃ©aire sur l'annÃ©e.
        On entraÃ®ne un modÃ¨le sur les donnÃ©es connues (colonne 'EspÃ©rance_Vie')
        puis on prÃ©dit pour les annÃ©es manquantes.

        :param df_final: DataFrame final avec colonnes CODE_DEP, AnnÃ©e, EspÃ©rance_Vie
        :return: DataFrame final avec les valeurs manquantes pour Mayotte complÃ©tÃ©es
                et arrondies Ã  2 dÃ©cimales.
        """
        from pyspark.ml.feature import VectorAssembler
        from pyspark.ml.regression import LinearRegression
        from pyspark.sql.functions import col, when, round

        # Filtrer uniquement les donnÃ©es de Mayotte
        df_mayotte = df_final.filter(col("CODE_DEP") == "976")

        # SÃ©parer les donnÃ©es connues et inconnues pour la colonne "EspÃ©rance_Vie"
        known = df_mayotte.filter(col("EspÃ©rance_Vie").isNotNull())
        unknown = df_mayotte.filter(col("EspÃ©rance_Vie").isNull())

        # PrÃ©parer les donnÃ©es pour la rÃ©gression
        assembler = VectorAssembler(inputCols=["AnnÃ©e"], outputCol="features")
        train = assembler.transform(known).select("AnnÃ©e", "features", "EspÃ©rance_Vie")

        # EntraÃ®ner le modÃ¨le de rÃ©gression linÃ©aire
        lr = LinearRegression(featuresCol="features", labelCol="EspÃ©rance_Vie")
        model = lr.fit(train)

        # PrÃ©dire pour les annÃ©es manquantes
        pred = assembler.transform(unknown)
        pred = model.transform(pred).select(
            "AnnÃ©e", col("prediction").alias("pred_value")
        )

        # Remplacer les valeurs manquantes par la prÃ©diction (arrondie Ã  2 dÃ©cimales)
        df_mayotte_filled = (
            df_mayotte.alias("base")
            .join(pred.alias("pred"), on="AnnÃ©e", how="left")
            .withColumn(
                "EspÃ©rance_Vie_new",
                when(
                    col("base.EspÃ©rance_Vie").isNull(), round(col("pred.pred_value"), 2)
                ).otherwise(round(col("base.EspÃ©rance_Vie"), 2)),
            )
            .select(
                col("base.CODE_DEP").alias("CODE_DEP"),
                col("base.AnnÃ©e").alias("AnnÃ©e"),
                col("EspÃ©rance_Vie_new").alias("EspÃ©rance_Vie"),
            )
        )

        # Conserver les donnÃ©es des autres dÃ©partements
        df_other = df_final.filter(col("CODE_DEP") != "976")

        # Fusionner et trier le DataFrame final
        return df_other.unionByName(df_mayotte_filled).orderBy("CODE_DEP", "AnnÃ©e")

    def transform_education_data(self, df):
        """
        Transforme et nettoie les donnÃ©es d'Ã©ducation issues du CSV 'fr-en-etablissements-fermes.csv'.
        Ã‰tapes de transformation :
          1. Suppression des doublons.
          2. Standardisation de toutes les colonnes textuelles (conversion en minuscules, suppression des espaces,
             remplacement des valeurs nulles par "non spÃ©cifiÃ©").
          3. Filtre des dÃ©partements mÃ©tropolitains (codes 1â€“95), puis format Ã  deux chiffres ("01", â€¦, "95").
          4. Conversion de la colonne "date_fermeture" en type Date et extraction de l'annÃ©e dans "annee_fermeture".
          5. Normalisation du code postal : remplacement des valeurs nulles par "00000", puis suppression des espaces.
          6. SÃ©paration des secteurs public et privÃ© Ã  partir de la colonne "secteur_public_prive_libe".
        """
        logger.info("ðŸš€ Transformation des donnÃ©es d'Ã©ducation en cours...")

        # 1. Suppression des doublons
        df = df.dropDuplicates()

        # 2. Standardisation des colonnes textuelles
        for colname in df.columns:
            df = df.withColumn(
                colname,
                F.when(
                    F.col(colname).isNotNull(), F.trim(F.lower(F.col(colname)))
                ).otherwise(F.lit("non spÃ©cifiÃ©")),
            )

        # 3. Filtrer et formater le code dÃ©partement
        if "code_departement" in df.columns:
            # extraire un entier si possible
            df = df.withColumn(
                "code_dept_int",
                F.when(
                    F.col("code_departement").rlike(r"^\d+$"),
                    F.col("code_departement").cast(IntegerType()),
                ).otherwise(None),
            )
            # ne garder que 1 â‰¤ code â‰¤ 95
            df = df.filter(
                (F.col("code_dept_int") >= 1) & (F.col("code_dept_int") <= 95)
            )
            # formater en "01", "02", â€¦, "95"
            df = df.withColumn(
                "code_departement",
                F.lpad(F.col("code_dept_int").cast("string"), 2, "0"),
            ).drop("code_dept_int")

        # 4. Conversion de 'date_fermeture' en DateType et extraction de l'annÃ©e
        if "date_fermeture" in df.columns:
            df = df.withColumn(
                "date_fermeture", F.col("date_fermeture").cast(DateType())
            )
            df = df.withColumn("annee_fermeture", F.year(F.col("date_fermeture")))

        # 5. Normalisation du code postal
        if "code_postal" in df.columns:
            df = df.withColumn(
                "code_postal",
                F.when(F.col("code_postal").isNull(), F.lit("00000")).otherwise(
                    F.trim(F.col("code_postal"))
                ),
            )

        # 6. SÃ©paration du secteur public/privÃ©
        if "secteur_public_prive_libe" in df.columns:
            df = df.withColumn(
                "secteur_public",
                F.when(F.col("secteur_public_prive_libe") == "public", 1).otherwise(0),
            ).withColumn(
                "secteur_prive",
                F.when(F.col("secteur_public_prive_libe") == "privÃ©", 1).otherwise(0),
            )

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "âœ… Transformation des donnÃ©es d'Ã©ducation rÃ©ussie.", df, 5
        )

    def calculate_closed_by_year_and_dept_education(self, df):
        """
        Calcule le nombre d'Ã©tablissements fermÃ©s par annÃ©e et par dÃ©partement (mÃ©tropole uniquement).
        Regroupe par 'annee_fermeture' et 'code_departement', puis agrÃ¨ge :
          - total d'Ã©tablissements,
          - fermetures publiques,
          - fermetures privÃ©es,
          - pourcentages publics/privÃ©s (2 dÃ©cimales).
        Pour les annÃ©es cibles [2002, 2007, 2012, 2017, 2022], complÃ¨te chaque dÃ©partement manquant par zÃ©ro.
        """
        import pyspark.sql.functions as F
        from pyspark.sql.functions import lit

        logger.info(
            "ðŸš€ Calcul des statistiques de fermetures d'Ã©tablissements par dÃ©partement et annÃ©e..."
        )

        # 1. AgrÃ©gation initiale
        df_grouped = df.groupBy("annee_fermeture", "code_departement").agg(
            F.first("libelle_departement").alias("libelle_departement"),
            F.count("numero_uai").alias("nombre_total_etablissements"),
            F.sum("secteur_public").alias("nb_public"),
            F.sum("secteur_prive").alias("nb_prive"),
        )

        # 2. Pourcentages
        df_grouped = df_grouped.withColumn(
            "pct_public",
            F.round(
                F.when(
                    F.col("nombre_total_etablissements") > 0,
                    F.col("nb_public") * 100.0 / F.col("nombre_total_etablissements"),
                ).otherwise(0.0),
                2,
            ),
        ).withColumn(
            "pct_prive",
            F.round(
                F.when(
                    F.col("nombre_total_etablissements") > 0,
                    F.col("nb_prive") * 100.0 / F.col("nombre_total_etablissements"),
                ).otherwise(0.0),
                2,
            ),
        )

        # 3. AnnÃ©es cibles
        target_years = list(range(2000, 2023))
        df_depts = (
            df.select("code_departement", "libelle_departement").distinct().cache()
        )
        result_dfs = []

        for year in target_years:
            df_year = df_depts.withColumn("annee_fermeture", lit(year))
            df_completed = df_year.join(
                df_grouped.filter(F.col("annee_fermeture") == year),
                on=["code_departement", "annee_fermeture", "libelle_departement"],
                how="left",
            ).na.fill(
                {
                    "nombre_total_etablissements": 0,
                    "nb_public": 0,
                    "nb_prive": 0,
                    "pct_public": 0.0,
                    "pct_prive": 0.0,
                }
            )
            result_dfs.append(df_completed)

        # 4. Conserver les autres annÃ©es
        df_other = df_grouped.filter(~F.col("annee_fermeture").isin(target_years))
        result_dfs.append(df_other)

        # 5. Union et nettoyage final
        df_completed = result_dfs[0]
        for part in result_dfs[1:]:
            df_completed = df_completed.unionByName(part, allowMissingColumns=True)

        df_completed = df_completed.na.fill(
            {
                "nombre_total_etablissements": 0,
                "nb_public": 0,
                "nb_prive": 0,
                "pct_public": 0.0,
                "pct_prive": 0.0,
            }
        ).orderBy("annee_fermeture", "code_departement")

        df_depts.unpersist()

        return self._extracted_from__extracted_from_combine_election_and_orientation_politique_52_116(
            "âœ… Calcul des statistiques complÃ©tÃ©. AperÃ§u :", df_completed, 10
        )

    def transform_security_data(self, df):
        """
        Transforme les donnÃ©es de sÃ©curitÃ© exclusivement avec pandas.
        Format de sortie: AnnÃ©e, DÃ©partement, DÃ©lits_total avec sÃ©parateur virgule.

        :param df: Non utilisÃ© dans cette version
        :return: DataFrame pandas avec colonnes (AnnÃ©e, DÃ©partement, DÃ©lits_total)
        """
        import pandas as pd
        from collections import defaultdict
        import os

        logger.info(
            "ðŸš€ Transformation des donnÃ©es de sÃ©curitÃ© avec pandas uniquement..."
        )

        try:
            # Chemin du fichier Excel
            fichier_excel = "data/origine/securite/tableaux-4001-ts.xlsx"

            if not os.path.exists(fichier_excel):
                logger.error(f"âŒ Fichier Excel non trouvÃ©: {fichier_excel}")
                return None

            logger.info(f"ðŸ“‚ Chargement du fichier Excel: {fichier_excel}")
            xls = pd.ExcelFile(fichier_excel)

            # Liste des feuilles correspondant aux dÃ©partements
            departements = [sheet for sheet in xls.sheet_names if sheet.isdigit()]
            logger.info(f"âœ“ {len(departements)} dÃ©partements identifiÃ©s")

            # Dictionnaire pour stocker les rÃ©sultats {(dÃ©partement, annÃ©e): total}
            resultats = defaultdict(int)

            # Plage d'annÃ©es ciblÃ©e
            annees_cibles = set(range(1996, 2023))  # de 1996 Ã  2022 inclus

            # Traitement de chaque feuille
            for dept in departements:
                try:
                    resultats = self._process_department_sheet(
                        dept, xls, annees_cibles, resultats
                    )
                except Exception as e:
                    logger.error(f"âŒ Erreur sur le dÃ©partement {dept}: {str(e)}")

            # CrÃ©ation du DataFrame final
            df_final = self._create_final_security_dataframe(resultats)

            logger.info(
                "âœ… Transformation des donnÃ©es de sÃ©curitÃ© terminÃ©e avec succÃ¨s"
            )
            return df_final

        except Exception as e:
            logger.error(f"âŒ Erreur lors de la transformation: {str(e)}")
            import traceback

            logger.error(traceback.format_exc())
            return None

    def _process_department_sheet(self, dept, xls, annees_cibles, resultats):
        """
        Traite une feuille de dÃ©partement et met Ã  jour le dictionnaire des rÃ©sultats.

        :param dept: Code du dÃ©partement
        :param xls: Objet ExcelFile pandas
        :param annees_cibles: Ensemble des annÃ©es Ã  considÃ©rer
        :param resultats: Dictionnaire des rÃ©sultats Ã  mettre Ã  jour
        :return: Dictionnaire des rÃ©sultats mis Ã  jour
        """
        import pandas as pd

        logger.info(f"âœ¨ Traitement du dÃ©partement {dept}...")
        df_dept = xls.parse(dept)
        df_dept = df_dept.dropna(how="all")  # retirer les lignes totalement vides

        for col in df_dept.columns:
            if isinstance(col, str) and col.startswith("_"):
                try:
                    annee = int(col.split("_")[1])
                    if annee in annees_cibles:
                        resultats[(annee, dept)] += df_dept[col].sum(skipna=True)
                except Exception as e:
                    logger.warning(f"âš ï¸ ProblÃ¨me avec la colonne {col}: {str(e)}")

        return resultats

    def _create_final_security_dataframe(self, resultats):
        """
        CrÃ©e le DataFrame final Ã  partir du dictionnaire des rÃ©sultats.

        :param resultats: Dictionnaire {(annÃ©e, dÃ©partement): total}
        :return: DataFrame pandas formatÃ©
        """
        import pandas as pd

        logger.info("âœ“ CrÃ©ation du DataFrame final...")
        df_final = pd.DataFrame(
            [
                {"AnnÃ©e": annee, "DÃ©partement": dept, "DÃ©lits_total": int(total)}
                for (annee, dept), total in resultats.items()
            ]
        )

        # Formater le code dÃ©partement pour avoir toujours 2 chiffres
        df_final["DÃ©partement"] = df_final["DÃ©partement"].apply(
            lambda x: x.zfill(2) if x.isdigit() and len(x) == 1 else x
        )

        # Tri pour lisibilitÃ© (d'abord par annÃ©e, puis par dÃ©partement)
        df_final = df_final.sort_values(by=["AnnÃ©e", "DÃ©partement"])

        # Afficher des statistiques
        if not df_final.empty:
            logger.info(
                f"âœ“ PÃ©riode couverte: de {df_final['AnnÃ©e'].min()} Ã  {df_final['AnnÃ©e'].max()}"
            )
            logger.info(f"âœ“ {df_final['DÃ©partement'].nunique()} dÃ©partements traitÃ©s")
            logger.info(f"âœ“ Total de {len(df_final)} lignes de donnÃ©es gÃ©nÃ©rÃ©es")

            # Ã‰chantillon des donnÃ©es
            logger.info(
                "âœ“ Ã‰chantillon des donnÃ©es (format: AnnÃ©e,DÃ©partement,DÃ©lits_total):"
            )
            for idx, row in df_final.head(5).iterrows():
                logger.info(
                    f"{row['AnnÃ©e']},{row['DÃ©partement']},{int(row['DÃ©lits_total'])}"
                )

        return df_final

    def transform_demography_data(self, df):
        """
        Transforme et nettoie le DataFrame de dÃ©mographie :
        - Trim & nettoyage du code dÃ©partement
        - Filtrage des lignes invalides
        - Conversion des colonnes en int
        - Tri par AnnÃ©e et Code_DÃ©partement
        """
        if df is None:
            logger.error("âŒ Le DataFrame de dÃ©mographie est vide ou invalide.")
            return None

        logger.info("ðŸš€ Transformation des donnÃ©es dÃ©mographiques en cours...")

        # 1) Nettoyage du Code_DÃ©partement
        df = df.withColumn("Code_DÃ©partement", trim(col("Code_DÃ©partement")))
        df = df.withColumn(
            "Code_DÃ©partement",
            regexp_replace(col("Code_DÃ©partement"), '"', "")
        )

        # 2) Filtrer uniquement les vrais codes (ex : 01, 2A, 2B, 75, ...)
        df = df.filter(col("Code_DÃ©partement").rlike("^(2A|2B|[0-9]{1,3})$"))

        # 3) Conversion de toutes les colonnes indicateurs et AnnÃ©e en int
        for c in df.columns:
            if c not in ("Code_DÃ©partement", "Nom_DÃ©partement"):
                df = df.withColumn(c, col(c).cast("int"))

        # 4) Tri final
        df = df.orderBy(col("AnnÃ©e").asc(), col("Code_DÃ©partement"))
        logger.info("âœ… Transformation dÃ©mographie terminÃ©e.")
        return df

    def combine_election_and_orientation_politique(self, df_election, df_orientation):
        """
        Combinaison des donnÃ©es Ã©lectorales avec les donnÃ©es d'orientation politique.
        """
        if df_election is None or df_orientation is None:
            logger.error("âŒ DonnÃ©es invalides pour la combinaison")

        logger.info(
            "ðŸš€ Combinaison des donnÃ©es Ã©lectorales avec les donnÃ©es d'orientation politique..."
        )

        # 3. Nettoyer les noms des candidats
        df_election = df_election.withColumn(
            "candidat_clean", trim(upper(col("candidat")))
        )

        # 4. Mapping candidat -> orientation politique
        candidate_to_orientation = {
            "CHARLES DE GAULLE": "droite",
            "FRANÃ‡OIS MITTERRAND": "gauche",
            "VALÃ‰RY GISCARD D'ESTAING": "centre droite",
            "VALÃ‰RY GISCARD DESTAING": "centre droite",
            "VALERY GISCARD D'ESTAING": "centre droite",
            "VALERY GISCARD DESTAING": "centre droite",
            "JACQUES CHIRAC": "droite",
            "LIONEL JOSPIN": "gauche",
            "NICOLAS SARKOZY": "droite",
            "SÃ‰GOLÃˆNE ROYAL": "gauche",
            "FRANÃ‡OIS HOLLANDE": "gauche",
            "MARINE LE PEN": "extreme droite",
            "JEAN-LUC MÃ‰LENCHON": "extreme gauche",
            "EMMANUEL MACRON": "centre",
            "ARLETTE LAGUILLER": "extreme gauche",
            "PHILIPPE POUTOU": "extreme gauche",
            "NATHALIE ARTHAUD": "extreme gauche",
            "JEAN-MARIE LE PEN": "extreme droite",
            "BENOÃŽT HAMON": "gauche",
            "DOMINIQUE DE VILLEPIN": "droite",
            "CHRISTINE BOUTIN": "droite",
            "FRANÃ‡OIS BAYROU": "centre droite",
            "NICOLAS DUPONT-AIGNAN": "droite",
            "Ã‰RIC ZEMMOUR": "extreme droite",
            "YANNICK JADOT": "Ã©cologiste",
            "NOÃ‹L MAMÃˆRE": "Ã©cologiste",
            "ANTOINE WAQUIN": "extreme gauche",
            "GEORGES MARCHAIS": "gauche",
            "ROBERT HUE": "gauche",
            "GEORGES POMPIDOU": "droite",
            "ALAIN POHER": "centre droite",
        }

        # 5. Ajouter la colonne orientation politique
        orientation_expr = create_map(
            [lit(k) for k in chain(*candidate_to_orientation.items())]
        )
        df_election = df_election.withColumn(
            "orientation_politique", orientation_expr.getItem(col("candidat_clean"))
        )

        # 6. CrÃ©er le mapping orientation -> id Ã  partir du fichier des partis
        orientation_id_map = {
            row["Orientation politique"]: row["id"]
            for row in df_orientation.select("Orientation politique", "id")
            .distinct()
            .collect()
        }
        orientation_id_expr = create_map(
            [lit(k) for k in chain(*orientation_id_map.items())]
        )
        df_election = df_election.withColumn(
            "id_parti", orientation_id_expr.getItem(col("orientation_politique"))
        )

        # Drop candidat_clean
        df_election = df_election.drop("candidat_clean")

        return self._extracted_from_combine_election_and_orientation_politique_52(
            "âœ… Combinaison des donnÃ©es Ã©lectorales avec les donnÃ©es d'orientation politique terminÃ©e",
            df_election,
            5,
        )

    # TODO Rename this here and in `transform_environmental_data`, `transform_pib_outre_mer`, `fill_missing_pib_mayotte`, `combine_all_pib_data`, `transform_inflation_data`, `combine_pib_and_inflation`, `transform_education_data`, `calculate_closed_by_year_and_dept_education`, `transform_demography_data` and `combine_election_and_orientation_politique`
    def _extracted_from_combine_election_and_orientation_politique_52(
        self, arg0, arg1, arg2
    ):
        return self._extracted_from__extracted_from_combine_election_and_orientation_politique_52_116(
            arg0, arg1, arg2
        )

    # TODO Rename this here and in `calculate_closed_by_year_and_dept_education`, `transform_demography_data` and `_extracted_from_combine_election_and_orientation_politique_52`
    def _extracted_from__extracted_from_combine_election_and_orientation_politique_52_116(
        self, arg0, arg1, arg2
    ):
        logger.info(arg0)
        arg1.show(arg2, truncate=False)
        return arg1

    # TODO Rename this here and in `calculate_closed_by_year_and_dept_education`, `transform_demography_data` and `_extracted_from_combine_election_and_orientation_politique_52`
    def _extracted_from__extracted_from_combine_election_and_orientation_politique_52_116(
        self, arg0, arg1, arg2
    ):
        logger.info(arg0)
        arg1.show(arg2, truncate=False)
        return arg1
