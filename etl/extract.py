# extract.py

import os
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType,
    StructField,
    IntegerType,
    StringType,
    DoubleType,
)
from pyspark.sql.functions import col, lit, regexp_extract
import glob
import logging
import traceback
from functools import reduce


# Configuration du logger
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def convert_excel_to_xlsx(file_xls, file_xlsx):
    """
    Convertit le fichier XLS en XLSX si ce dernier n'existe pas d√©j√†.
    """
    if not os.path.exists(file_xlsx):
        print("üì• Conversion du fichier XLS en XLSX...")
        try:
            # Charger toutes les feuilles du fichier XLS
            excel_data = pd.read_excel(file_xls, sheet_name=None)
            with pd.ExcelWriter(file_xlsx, engine="openpyxl") as writer:
                for sheet, data in excel_data.items():
                    data.to_excel(writer, sheet_name=sheet, index=False)
            print(f"‚úÖ Conversion r√©ussie : {file_xlsx}")
        except Exception as e:
            print(f"‚ùå Erreur lors de la conversion : {e}")
    else:
        print(f"‚úì {file_xlsx} existe d√©j√†. Conversion ignor√©e.")


def extract_and_merge_excel(file_xlsx, file_csv):
    """
    Extrait et fusionne les feuilles Excel (sauf la premi√®re) en un unique CSV.
    Les feuilles sont lues en ignorant les trois premi√®res lignes (skiprows=3),
    et une colonne "Ann√©e" est ajout√©e √† partir du nom de la feuille.
    Le CSV est √©crit avec le s√©parateur ';'.
    """
    if not os.path.exists(file_csv):
        print("üì• Extraction et fusion des feuilles Excel...")
        try:
            df_final = _process_excel_sheets(file_xlsx)
            # Sauvegarder en CSV avec le s√©parateur point-virgule
            df_final.to_csv(file_csv, index=False, sep=";")
            print(f"‚úÖ Fichier CSV g√©n√©r√© : {file_csv}")
        except Exception as e:
            print(f"‚ùå Erreur lors de l'extraction/fusion : {e}")
    else:
        print(f"‚úì {file_csv} existe d√©j√†. Extraction ignor√©e.")
    
    
def _process_excel_sheets(file_xlsx):
    """
    Traite les feuilles Excel en ignorant la premi√®re feuille (ex: "√Ä savoir").
    Retourne un DataFrame combin√© avec une colonne Ann√©e ajout√©e.
    """
    # Charger le classeur XLSX
    xls = pd.ExcelFile(file_xlsx)
    # On ignore la premi√®re feuille (ex: "√Ä savoir")
    sheets_to_read = xls.sheet_names[1:]
    dfs = []
    for sheet in sheets_to_read:
        print(f"üìÑ Traitement de la feuille : {sheet}")
        df = pd.read_excel(xls, sheet_name=sheet, skiprows=3)
        df["Ann√©e"] = sheet  # Ajouter l'ann√©e issue du nom de la feuille
        dfs.append(df)
    return pd.concat(dfs, ignore_index=True)


class DataExtractor:
    """
    Classe permettant d'extraire les donn√©es des fichiers CSV pour l'ETL.
    """

    def __init__(self, app_name="EnvironmentalDataETL", master="local[*]"):
        """Initialise une session Spark."""
        logger.info("üöÄ Initialisation de la session Spark...")
        self.spark = self._create_spark_session(app_name, master)
        self.spark.sparkContext.setLogLevel("ERROR")  # R√©duction des logs Spark
        logger.info("‚úÖ Session Spark initialis√©e avec succ√®s")

    def _create_spark_session(self, app_name, master):
        """Cr√©e et configure une session Spark."""
        return (
            SparkSession.builder.appName(app_name)
            .master(master)
            .config("spark.driver.host", "127.0.0.1")
            .config(
                "spark.python.worker.reuse", "true"
            )  # R√©utiliser les workers Python
            .config("spark.python.worker.timeout", "1800")  # Augmenter le timeout
            .config(
                "spark.driver.extraClassPath",
                "./database/connector/mysql-connector-j-9.1.0.jar;./database/connector/spark-excel_2.12-3.5.0_0.20.3.jar",
            )
            .config("spark.hadoop.fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem")
            .config("spark.driver.memory", "8g")
            .config("spark.executor.memory", "8g")
            .config("spark.memory.offHeap.enabled", "true")
            .config("spark.memory.offHeap.size", "8g")
            # Ajouter le chemin Python explicite
            .config(
                "spark.pyspark.python", "python"
            )  # ou le chemin complet vers votre ex√©cutable Python
            .config("spark.pyspark.driver.python", "python")  # ou le chemin complet
            .getOrCreate()
        )

    def extract_environmental_data(self, file_path):
        """
        Charge les donn√©es du fichier "parc-regional-annuel-prod-eolien-solaire.csv".

        :param file_path: Chemin du fichier CSV √† charger
        :return: DataFrame PySpark contenant les donn√©es environnementales
        """
        if not os.path.exists(file_path):
            logger.error(f"‚ùå Fichier non trouv√© : {file_path}")
            return None

        logger.info(f"üìå Extraction des donn√©es environnementales depuis : {file_path}")

        try:
            df = self._load_environmental_data(file_path)
            df = self._normalize_column_names(df)
            return df
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction des donn√©es : {str(e)}")
            return None

    def _load_environmental_data(self, file_path):
        """Charge le fichier CSV avec un sch√©ma sp√©cifique."""
        schema = StructType(
            [
                StructField("Ann√©e", IntegerType(), True),
                StructField("Code INSEE r√©gion", IntegerType(), True),
                StructField("R√©gion", StringType(), True),
                StructField("Parc install√© √©olien (MW)", DoubleType(), True),
                StructField("Parc install√© solaire (MW)", DoubleType(), True),
                StructField("G√©o-shape r√©gion", StringType(), True),
                StructField("G√©o-point r√©gion", StringType(), True),
            ]
        )

        return (
            self.spark.read.option("header", "true")
            .option("delimiter", ";")
            .option("enforceSchema", "true")
            .schema(schema)
            .csv(file_path)
        )

    def _normalize_column_names(self, df):
        """Normalise les noms de colonnes."""
        return (
            df.withColumnRenamed("Code INSEE r√©gion", "Code_INSEE_r√©gion")
            .withColumnRenamed("Parc install√© √©olien (MW)", "Parc_install√©_√©olien_MW")
            .withColumnRenamed("Parc install√© solaire (MW)", "Parc_install√©_solaire_MW")
            .withColumnRenamed("G√©o-shape r√©gion", "G√©o_shape_r√©gion")
            .withColumnRenamed("G√©o-point r√©gion", "G√©o_point_r√©gion")
        )

    def extract_pib_outre_mer(self, file_paths):
        """
        Extrait et combine les donn√©es PIB brutes des fichiers CSV outre-mer.
        """
        schema = StructType(
            [
                StructField("Ann√©e", StringType(), True),
                StructField("PIB_en_euros_par_habitant", StringType(), True),
                StructField("Codes", StringType(), True),
            ]
        )

        dfs = []
        for path in file_paths:
            if os.path.exists(path):
                df = (
                    self.spark.read.option("header", "true")
                    .option("delimiter", ";")
                    .schema(schema)
                    .csv(path)
                )
                # Ajoute une colonne temporaire indiquant le fichier source
                df = df.withColumn("source_file", lit(path))
                dfs.append(df)
            else:
                logger.error(f"‚ùå Fichier non trouv√© : {path}")

        if not dfs:
            logger.error("‚ùå Aucun fichier PIB valide trouv√©.")
            return None

        return reduce(lambda df1, df2: df1.union(df2), dfs)

    def extract_pib_excel(self, excel_path):
        """
        Extrait les donn√©es PIB r√©gionales √† partir du fichier Excel (1990-2021).
        """
        if not os.path.exists(excel_path):
            logger.error(f"‚ùå Fichier non trouv√© : {excel_path}")
            return None

        logger.info(f"üì• Extraction PIB Excel : {excel_path}")

        try:
            df = self._load_pib_excel_data(excel_path)
            df = self._select_pib_columns(df)
            return df
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction Excel : {str(e)}")
            return None

    def _load_pib_excel_data(self, excel_path):
        """Charge les donn√©es PIB depuis le fichier Excel avec un sch√©ma d√©fini."""
        schema = StructType(
            [
                StructField("Code_INSEE_R√©gion", StringType(), True),
                StructField("libgeo", StringType(), True),
                StructField("Ann√©e", IntegerType(), True),
                StructField("PIB_en_euros_par_habitant", IntegerType(), True),
            ]
        )
        return (
            self.spark.read.format("com.crealytics.spark.excel")
            .option("header", "true")
            .option("dataAddress", "'Data'!A5")
            .schema(schema)
            .load(excel_path)
        )

    def _select_pib_columns(self, df):
        """S√©lectionne les colonnes pertinentes pour les donn√©es PIB."""
        return df.select("Ann√©e", "PIB_en_euros_par_habitant", "Code_INSEE_R√©gion")

    def extract_pib_2022(self, csv_path):
        """
        Extrait le fichier PIB 2022 r√©gional CSV.
        """

        if not os.path.exists(csv_path):
            logger.error(f"‚ùå Fichier non trouv√© : {csv_path}")
            return None

        logger.info(f"üì• Extraction PIB 2022 depuis : {csv_path}")

        try:
            df_raw = self._load_pib_2022_data(csv_path)
            return self._select_and_rename_pib_2022_columns(df_raw)
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction PIB 2022: {str(e)}")
            return None

    def _load_pib_2022_data(self, csv_path):
        """Charge les donn√©es PIB 2022 depuis le fichier CSV avec un sch√©ma d√©fini."""
        schema = StructType(
            [
                StructField("Code_INSEE_R√©gion", StringType(), True),
                StructField("Libell√©", StringType(), True),
                StructField("PIB_en_euros_par_habitant", IntegerType(), True),
            ]
        )
        return (
            self.spark.read.option("header", "true")
            .option("delimiter", ";")
            .schema(schema)
            .csv(csv_path)
        )

    def _select_and_rename_pib_2022_columns(self, df):
        """S√©lectionne et renomme les colonnes pour les donn√©es PIB 2022."""
        return df.select(
            col("Code_INSEE_R√©gion"),
            lit(2022).alias("Ann√©e"),
            col("PIB_en_euros_par_habitant"),
        )

    def extract_inflation_data(self, excel_path):
        """
        Extrait les donn√©es d'inflation depuis le fichier Excel.

        :param excel_path: Chemin du fichier Excel contenant les donn√©es d'inflation.
        :return: DataFrame PySpark contenant les donn√©es d'inflation filtr√©es de 2000 √† 2022.
        """
        if not os.path.exists(excel_path):
            logger.error(f"‚ùå Fichier non trouv√© : {excel_path}")
            return None

        logger.info(f"üì• Extraction des donn√©es d'inflation depuis : {excel_path}")

        # D√©finition du sch√©ma explicitement pour correspondre √† la ligne d'en-t√™te effective
        schema = StructType(
            [
                StructField("Ann√©e", IntegerType(), True),
                StructField("√âvolution des prix √† la consommation", DoubleType(), True),
            ]
        )

        try:
            return self._extracted_from_extract_inflation_data_23(excel_path, schema)
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction Excel inflation : {str(e)}")
            return None

    def _extracted_from_extract_inflation_data_23(self, excel_path, schema):
        df = self._load_inflation_data(excel_path, schema)
        df = self._rename_inflation_column(df)
        logger.info(f"üõ†Ô∏è Colonnes apr√®s extraction et renommage : {df.columns}")
        return self._filter_inflation_years(df)

    def _load_inflation_data(self, excel_path, schema):
        """Charge les donn√©es d'inflation depuis Excel avec le sch√©ma sp√©cifi√©."""
        return (
            self.spark.read.format("com.crealytics.spark.excel")
            .option("header", "true")
            .option("sheetName", "Question 1")
            .option("dataAddress", "'Question 1'!A4")
            .schema(schema)
            .load(excel_path)
        )

    def _rename_inflation_column(self, df):
        """Renomme la colonne d'inflation pour supprimer les espaces."""
        return df.withColumnRenamed(
            "√âvolution des prix √† la consommation",
            "√âvolution_des_prix_√†_la_consommation",
        )

    def _filter_inflation_years(self, df):
        """Filtre les donn√©es d'inflation pour les ann√©es entre 2000 et 2022."""
        return df.filter((col("Ann√©e") >= 2000) & (col("Ann√©e") <= 2022))

    def extract_technologie_data(self, excel_path):
        """
        Extrait les donn√©es de technologie depuis le fichier Excel.

        :param excel_path: Chemin du fichier Excel contenant les donn√©es de technologie
        :return: DataFrame PySpark contenant les donn√©es brutes
        """
        if not os.path.exists(excel_path):
            logger.error(f"‚ùå Fichier non trouv√© : {excel_path}")
            return None

        logger.info(f"üì• Extraction des donn√©es de technologie depuis : {excel_path}")

        try:
            df = (
                self.spark.read.format("com.crealytics.spark.excel")
                .option("header", "true")
                .option("inferSchema", "true")
                .option("dataAddress", "'Tableau 1'!A3:B37")
                .load(excel_path)
            )

            logger.info("‚úÖ Extraction des donn√©es de technologie r√©ussie")
            return df

        except Exception as e:
            logger.error(
                f"‚ùå Erreur lors de l'extraction des donn√©es de technologie : {str(e)}"
            )
            return None

    def extract_election_data_1965_2012(self, file_pattern):
        """
        Extrait les donn√©es √©lectorales des fichiers CSV de 1965 √† 2012.

        :param file_pattern: Motif des fichiers CSV √† traiter (glob)
        :return: Liste de DataFrames PySpark contenant les donn√©es brutes
        """
        logger.info(
            f"üì• Extraction des donn√©es √©lectorales 1965-2012 depuis : {file_pattern}"
        )
        file_list = glob.glob(file_pattern)
        results = []

        for file_path in file_list:
            try:
                # Lecture du fichier CSV avec en-t√™te et sch√©ma inf√©r√©
                df = (
                    self.spark.read.option("header", "true")
                    .option("inferSchema", "true")
                    .csv(file_path)
                )

                # Ajout des colonnes ann√©e et nom de fichier
                df = df.withColumn("filename", lit(file_path))
                df = df.withColumn(
                    "annee", regexp_extract("filename", r"presi(\d{4})", 1)
                )

                results.append(df)
            except Exception as e:
                logger.error(
                    f"‚ùå Erreur lors de l'extraction du fichier {file_path}: {str(e)}"
                )
                continue

        if not results:
            logger.warning("Aucun fichier CSV trouv√© pour 1965-2012.")
            return None

        return results

    def extract_election_data_2017(self, excel_file):
        """
        Extrait les donn√©es √©lectorales du fichier Excel 2017.
        """
        logger.info(f"üì• Extraction des donn√©es √©lectorales 2017 : {excel_file}")
        try:
            return (
                self.spark.read.format("com.crealytics.spark.excel")
                .option("header", "true")
                .option("inferSchema", "true")
                .option("dataAddress", "'D√©partements Tour 2'!A3:Z115")
                .load(excel_file)
            )
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction 2017 : {str(e)}")
            return None

    def extract_election_data_2022(self, excel_file):
        """
        Extrait les donn√©es √©lectorales du fichier Excel 2022.
        """
        logger.info(f"üì• Extraction des donn√©es √©lectorales 2022 : {excel_file}")
        try:
            return (
                self.spark.read.format("com.crealytics.spark.excel")
                .option("header", "true")
                .option("inferSchema", "true")
                .option("sheetName", "R√©sultats")
                .load(excel_file)
            )
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction 2022 : {str(e)}")
            return None

    def extract_life_expectancy_data(self, file_path):
        """
        Charge le fichier CSV 'valeurs_anuelles.csv' contenant les donn√©es d'esp√©rance de vie √† la naissance.
        Le fichier est structur√© avec une ligne d'en-t√™te contenant :
        "Libell√©";"idBank";"Derni√®re mise √† jour";"P√©riode";"1901";"1902"; ... ;"2024"

        :param file_path: Chemin du fichier CSV √† charger
        :return: DataFrame PySpark contenant les donn√©es brutes d'esp√©rance de vie
        """
        if not os.path.exists(file_path):
            logger.error(f"‚ùå Fichier non trouv√© : {file_path}")
            return None

        logger.info(
            f"üì• Extraction des donn√©es d'esp√©rance de vie depuis : {file_path}"
        )

        try:
            return (
                self.spark.read.option("header", "true")
                .option("delimiter", ";")
                .option("inferSchema", "true")
                .csv(file_path)
            )
        except Exception as e:
            logger.error(
                f"‚ùå Erreur lors de l'extraction des donn√©es d'esp√©rance de vie : {str(e)}"
            )
            return None

    def extract_departments_data(self, file_path):
        """
        Extrait les donn√©es des d√©partements depuis le fichier CSV "departements-france.csv".
        Le fichier contient les colonnes : code_departement, nom_departement, code_region, nom_region.

        :param file_path: Chemin du fichier CSV des d√©partements.
        :return: DataFrame PySpark avec les donn√©es des d√©partements.
        """
        if not os.path.exists(file_path):
            logger.error(f"‚ùå Fichier de d√©partements non trouv√© : {file_path}")
            return None

        logger.info(f"üì• Extraction des donn√©es de d√©partements depuis : {file_path}")

        schema = StructType(
            [
                StructField("code_departement", StringType(), True),
                StructField("nom_departement", StringType(), True),
                StructField("code_region", StringType(), True),
                StructField("nom_region", StringType(), True),
            ]
        )

        try:
            return (
                self.spark.read.option("header", "true")
                .option("delimiter", ",")
                .schema(schema)
                .csv(file_path)
            )
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction des d√©partements : {str(e)}")
            return None

    def extract_education_data(self, input_path: str):
        """
        Extrait les donn√©es d'√©ducation depuis un fichier CSV.
        Le fichier est attendu avec un en-t√™te, un s√©parateur ';' et un encodage UTF-8.

        :param input_path: Chemin du fichier CSV d'√©ducation.
        :return: DataFrame PySpark contenant les donn√©es d'√©ducation.
        """
        if not os.path.exists(input_path):
            logger.error(f"‚ùå Fichier d'√©ducation non trouv√© : {input_path}")
            return None

        logger.info(f"üì• Extraction des donn√©es d'√©ducation depuis : {input_path}")
        try:
            df = (
                self.spark.read.option("header", "true")
                .option("sep", ";")
                .option("encoding", "UTF-8")
                .csv(input_path)
            )
            logger.info(
                f"‚úì Donn√©es d'√©ducation charg√©es avec succ√®s depuis {input_path}"
            )
            logger.info(f"‚úÖ Nombre de lignes: {df.count()}")
            logger.info(f"‚úÖ Colonnes pr√©sentes: {', '.join(df.columns)}")
            return df
        except Exception as e:
            logger.error(
                f"‚ùå Erreur lors du chargement du fichier d'√©ducation : {str(e)}"
            )
            return None

    def extract_security_data(self, excel_path):
        """
        Extrait les donn√©es de s√©curit√© directement avec pandas sans utiliser Spark.
        Lit chaque feuille (d√©partement) du fichier Excel des tableaux 4001.

        :param excel_path: Chemin du fichier Excel contenant les donn√©es de s√©curit√©
        :return: DataFrame pandas avec les donn√©es brutes
        """
        if not os.path.exists(excel_path):
            logger.error(f"‚ùå Fichier non trouv√© : {excel_path}")
            return None

        logger.info(f"üì• Extraction des donn√©es de s√©curit√© depuis : {excel_path}")

        try:
            import pandas as pd

            # Lire les noms des feuilles
            xls = pd.ExcelFile(excel_path)
            dept_sheets = [sheet for sheet in xls.sheet_names if sheet.isdigit()]

            logger.info(
                f"‚úì {len(dept_sheets)} d√©partements identifi√©s dans le fichier Excel"
            )

            # Liste pour stocker les DataFrames pandas
            all_dfs = []

            # Lire chaque feuille avec pandas
            for dept in dept_sheets:
                logger.info(f"ü§ñ Lecture de la feuille du d√©partement {dept}")
                try:
                    df_sheet = pd.read_excel(excel_path, sheet_name=dept)
                    df_sheet["departement"] = dept
                    all_dfs.append(df_sheet)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Probl√®me avec la feuille {dept}: {str(e)}")
                    continue

            # Combiner tous les DataFrames pandas
            if not all_dfs:
                logger.error("‚ùå Aucune donn√©e trouv√©e dans le fichier Excel")
                return None

            df_combined = pd.concat(all_dfs, ignore_index=True)
            logger.info(
                f"‚úÖ Extraction r√©ussie: {len(df_combined)} lignes, {len(df_combined.columns)} colonnes"
            )

            # Information sur les colonnes d'ann√©es
            year_cols = [
                col
                for col in df_combined.columns
                if isinstance(col, str) and col.startswith("_")
            ]
            logger.info(f"‚úÖ {len(year_cols)} colonnes d'ann√©es identifi√©es")

            return df_combined

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction: {str(e)}")
            import traceback

            logger.error(traceback.format_exc())
            return None

    def extract_demography_data(self, file_xls, file_xlsx, file_csv):
        """
        Convertit le fichier XLS de d√©mographie en XLSX, fusionne les feuilles en un CSV,
        puis charge les donn√©es dans un DataFrame Spark.
        Cette m√©thode reprend le code de conversion et d'extraction pr√©sent dans transform_demoV2.
        """
        # --- Conversion XLS -> XLSX ---
        if not os.path.exists(file_xlsx):
            print("üì• Conversion du fichier XLS en XLSX...")
            try:
                excel_data = pd.read_excel(file_xls, sheet_name=None)
                with pd.ExcelWriter(file_xlsx, engine="openpyxl") as writer:
                    for sheet, data in excel_data.items():
                        data.to_excel(writer, sheet_name=sheet, index=False)
                print(f"‚úÖ Conversion r√©ussie : {file_xlsx}")
            except Exception as e:
                print(f"‚ùå Erreur lors de la conversion : {e}")
        else:
            print(f"‚úì {file_xlsx} existe d√©j√†. Conversion ignor√©e.")

        # --- Extraction et fusion des feuilles en CSV ---
        if not os.path.exists(file_csv):
            print("üì• Extraction et fusion des feuilles Excel...")
            try:
                df_final = self._extract_and_combine_demo_sheets(file_xlsx)
                # Sauvegarder en CSV avec le s√©parateur point-virgule
                df_final.to_csv(file_csv, index=False, sep=";")
                print(f"‚úÖ Fichier CSV g√©n√©r√© : {file_csv}")
            except Exception as e:
                print(f"‚ùå Erreur lors de l'extraction/fusion : {e}")
        else:
            print(f"‚úì {file_csv} existe d√©j√†. Extraction ignor√©e.")

        # --- Chargement du CSV avec Spark ---
        return self.spark.read.option("header", True).option("sep", ";").csv(file_csv)

    def extract_orientation_politique(self, file_path):
        """
        Extrait les donn√©es d'orientation politique depuis un fichier CSV.
        """
        if not os.path.exists(file_path):
            logger.error(f"‚ùå Fichier non trouv√© : {file_path}")
            return None
            
        logger.info(f"üì• Extraction des donn√©es d'orientation politique depuis : {file_path}")
        return self.spark.read.option("header", True).csv(file_path)

    def stop(self):
        """
        Arr√™te la session Spark si elle est active.
        """
        if self.spark:
            self.spark.stop()
            logger.info("ü§ñ‚úÖ Session Spark arr√™t√©e proprement.")
