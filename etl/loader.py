import logging
import os
import shutil
import pandas as pd
from typing import Optional
import mysql.connector
from dotenv import load_dotenv

# Chargement des variables d'environnement
load_dotenv()

# Configuration du logger
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DataLoader:
    """
    Classe permettant d'enregistrer un DataFrame PySpark transform√© en fichier CSV
    ou de les charger dans une base de donn√©es MySQL.
    """

    def __init__(self, spark, output_dir="data\processed_data", 
                 jdbc_url=None, user=None, password=None, database=None, 
                 driver="com.mysql.cj.jdbc.Driver"):
        """
        Initialise le DataLoader avec un r√©pertoire de sortie et des param√®tres de connexion √† MySQL.

        :param spark: Session Spark
        :param output_dir: Dossier o√π seront stock√©s les fichiers CSV transform√©s
        :param jdbc_url: URL JDBC pour la connexion MySQL
        :param user: Nom d'utilisateur MySQL
        :param password: Mot de passe MySQL
        :param database: Base de donn√©es MySQL
        :param driver: Driver JDBC √† utiliser
        """
        logger.info(
            f"üöÄ Initialisation du DataLoader avec le dossier de sortie : {output_dir}"
        )
        self.spark = spark
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)  # Cr√©e le dossier s'il n'existe pas
        
        # Param√®tres de connexion MySQL
        self.jdbc_url = jdbc_url or os.getenv("MYSQL_JDBC_URL", "jdbc:mysql://localhost:3306")
        self.user = user or os.getenv("MYSQL_USER", "root")
        self.password = password or os.getenv("MYSQL_PASSWORD", "")
        self.database = database or os.getenv("MYSQL_DATABASE", "elections_presidentielles")
        self.driver = driver
        
        logger.info("‚úÖ Dossier de sortie cr√©√©/valid√©")
        logger.info(f"‚úÖ Configuration MySQL: {self.jdbc_url}/{self.database}")

    def save_to_csv(self, df, input_file_path):
        """
        Sauvegarde un DataFrame en fichier CSV apr√®s transformation.
        Compatible avec les DataFrames pandas et Spark.
        """
        if df is None:
            logger.error("‚ùå Impossible de sauvegarder un DataFrame vide.")
            return

        # Normaliser le chemin d'entr√©e
        input_file_path = os.path.normpath(input_file_path)
        base_name = os.path.basename(input_file_path).replace(".csv", "_processed.csv")
        final_output_path = os.path.normpath(os.path.join(self.output_dir, base_name))

        logger.info(
            f"‚ö° Enregistrement des donn√©es transform√©es dans : {final_output_path}"
        )

        try:
            # V√©rifier si c'est un DataFrame pandas ou Spark
            if hasattr(df, "toPandas"):  # C'est un DataFrame Spark
                # Utiliser la m√©thode originale pour Spark
                df = df.coalesce(1)
                df.write.mode("overwrite").option("header", "true").option(
                    "delimiter", ";"  # Utiliser la virgule comme s√©parateur
                ).csv(final_output_path + "_temp")

                if temp_file := next(
                    (
                        os.path.join(final_output_path + "_temp", filename)
                        for filename in os.listdir(final_output_path + "_temp")
                        if filename.endswith(".csv")
                    ),
                    None,
                ):
                    shutil.copy2(temp_file, final_output_path)
                    shutil.rmtree(final_output_path + "_temp")
                    logger.info("‚úÖ Fichier CSV sauvegard√© avec succ√®s !")
                else:
                    logger.error(
                        "‚ùå Aucun fichier CSV g√©n√©r√© dans le dossier temporaire."
                    )
            else:  # C'est un DataFrame pandas
                # Pour le fichier de s√©curit√©, on utilise toujours le header
                df.to_csv(final_output_path, sep=";", index=False, header=True)
                logger.info("‚úÖ Fichier CSV sauvegard√© avec succ√®s via pandas!")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'enregistrement du fichier : {str(e)}")
            if os.path.exists(final_output_path + "_temp"):
                shutil.rmtree(final_output_path + "_temp")
    
    # M√©thodes de chargement des donn√©es vers MySQL
    def load_dim_politique(self, df_dim_politique, mode="append"):
        """Charge les donn√©es de la dimension politique dans MySQL"""
        self._load(df_dim_politique, "dim_politique", mode)
        logger.info("‚ú® Chargement des donn√©es dans la table dim_politique termin√©")
    
    def load_dim_securite(self, df_dim_securite, mode="append"):
        """Charge les donn√©es de la dimension s√©curit√© dans MySQL"""
        self._load(df_dim_securite, "dim_securite", mode)
        logger.info("‚ú® Chargement des donn√©es dans la table dim_securite termin√©")
    
    def load_dim_sante(self, df_dim_sante, mode="append"):
        """Charge les donn√©es de la dimension sant√© dans MySQL"""
        self._load(df_dim_sante, "dim_sante", mode)
        logger.info("‚ú® Chargement des donn√©es dans la table dim_sante termin√©")
    
    def load_dim_education(self, df_dim_education, mode="append"):
        """Charge les donn√©es de la dimension √©ducation dans MySQL"""
        self._load(df_dim_education, "dim_education", mode)
        logger.info("‚ú® Chargement des donn√©es dans la table dim_education termin√©")
    
    def load_dim_environnement(self, df_dim_environnement, mode="append"):
        """Charge les donn√©es de la dimension environnement dans MySQL"""
        self._load(df_dim_environnement, "dim_environnement", mode)
        logger.info("‚ú® Chargement des donn√©es dans la table dim_environnement termin√©")
    
    def load_dim_socio_economie(self, df_dim_socio_economie, mode="append"):
        """Charge les donn√©es de la dimension socio-√©conomie dans MySQL"""
        self._load(df_dim_socio_economie, "dim_socio_economie", mode)
        logger.info("‚ú® Chargement des donn√©es dans la table dim_socio_economie termin√©")
    
    def load_dim_technologie(self, df_dim_technologie, mode="append"):
        """Charge les donn√©es de la dimension technologie dans MySQL"""
        self._load(df_dim_technologie, "dim_technologie", mode)
        logger.info("‚ú® Chargement des donn√©es dans la table dim_technologie termin√©")
    
    def load_dim_demographie(self, df_dim_demographie, mode="append"):
        """Charge les donn√©es de la dimension d√©mographie dans MySQL"""
        self._load(df_dim_demographie, "dim_demographie", mode)
        logger.info("‚ú® Chargement des donn√©es dans la table dim_demographie termin√©")
    
    def load_fact_resultats_politique(self, df_fact_resultats_politique, mode="append"):
        """Charge les donn√©es de la table de faits r√©sultats_politique dans MySQL"""
        self._load(df_fact_resultats_politique, "fact_resultats_politique", mode)
        logger.info("‚ú® Chargement des donn√©es dans la table fact_resultats_politique termin√©")

    def _load(self, df, table_name, mode):
        """
        R√©alise l'√©criture JDBC dans MySQL.
        :param df: DataFrame √† charger
        :param table_name: Nom de la table cible
        :param mode: Mode d'insertion ('append', 'overwrite', etc.)
        """
        props = {
            "user": self.user,
            "password": self.password,
            "driver": self.driver
        }
        
        # D√©sactivation temporaire des contraintes FK
        try:
            conn = mysql.connector.connect(
                host=self._get_host_from_jdbc(),
                database=self.database,
                user=self.user,
                password=self.password
            )
            cursor = conn.cursor()
            self._execute_sql(cursor, "SET FOREIGN_KEY_CHECKS=0;", conn, "Foreign key checks disabled.")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la d√©sactivation des contraintes FK: {str(e)}")
            conn = None

        try:
            # Construire l'URL compl√®te avec la base de donn√©es
            full_jdbc_url = f"{self.jdbc_url}/{self.database}"
            
            # Si le mode est 'overwrite', utiliser TRUNCATE au lieu de DROP
            if mode == "overwrite" and conn is not None:
                try:
                    # Tronquer la table au lieu de la supprimer
                    self._execute_sql(cursor, f"TRUNCATE TABLE {table_name};", conn, f"Table {table_name} truncated.")
                    # Passer en mode append pour √©viter que Spark ne tente de DROP la table
                    mode = "append"
                except Exception as ex:
                    logger.warning(f"Impossible de tronquer la table {table_name}: {ex}")
            
            # √âcriture des donn√©es
            df.write.jdbc(
                url=full_jdbc_url,
                table=table_name,
                mode=mode,
                properties=props
            )
            logger.info(f"‚ö° Donn√©es charg√©es dans la table {table_name} avec succ√®s.")
            
            # R√©activer les contraintes FK
            if conn:
                self._execute_sql(cursor, "SET FOREIGN_KEY_CHECKS=1;", conn, "Foreign key checks enabled.")
                conn.close()
        except Exception as e:
            logger.error(f"‚õî Erreur lors du chargement dans la table {table_name}: {str(e)}")
            if conn:
                try:
                    self._execute_sql(cursor, "SET FOREIGN_KEY_CHECKS=1;", conn, "Foreign key checks re-enabled.")
                    conn.close()
                except:
                    pass


    def _execute_sql(self, cursor, sql_statement, conn, success_message):
        """
        Ex√©cute une instruction SQL et valide la transaction.
        
        :param cursor: Curseur MySQL
        :param sql_statement: Instruction SQL √† ex√©cuter
        :param conn: Connexion MySQL
        :param success_message: Message de succ√®s √† afficher
        """
        cursor.execute(sql_statement)
        conn.commit()
        logger.info(success_message)

    def _get_host_from_jdbc(self):
        """
        Extrait le nom d'h√¥te de l'URL JDBC.
        
        :return: Nom d'h√¥te
        """
        try:
            url_without_prefix = self.jdbc_url.split("://")[1]
            host_port = url_without_prefix.split("/")[0]
            return host_port.split(":")[0]
        except Exception:
            return "localhost"
    
    def initialize_database(self):
        """
        Initialise la base de donn√©es en cr√©ant les tables si elles n'existent pas.
        """
        try:
            conn = mysql.connector.connect(
                host=self._get_host_from_jdbc(),
                user=self.user,
                password=self.password
            )
            cursor = conn.cursor()
            
            # Cr√©ation de la base de donn√©es si elle n'existe pas
            self._execute_sql(
                cursor,
                f"CREATE DATABASE IF NOT EXISTS {self.database};",
                conn,
                f"‚úÖ Base de donn√©es '{self.database}' cr√©√©e ou d√©j√† existante."
            )
            
            # Utiliser la base de donn√©es
            self._execute_sql(cursor, f"USE {self.database};", conn, f"‚úÖ Utilisation de la base '{self.database}'.")
            
            # Cr√©ation des tables selon le sch√©ma
            # Table dim_politique
            self._execute_sql(cursor, """
            CREATE TABLE IF NOT EXISTS `dim_politique` (
              `id` INT PRIMARY KEY,
              `etiquette_parti` INT,
              `annee` INT,
              `code_dept` VARCHAR(3),
              `candidat` VARCHAR(100),
              `total_voix` INT,
              `orientation_politique` VARCHAR(50)
            );
            """, conn, "‚úÖ Table dim_politique cr√©√©e ou d√©j√† existante.")
            
            # Table dim_securite
            self._execute_sql(cursor, """
            CREATE TABLE IF NOT EXISTS `dim_securite` (
              `id` INT PRIMARY KEY AUTO_INCREMENT,
              `annee` INT,
              `code_dept` VARCHAR(3),
              `delits_total` INT
            );
            """, conn, "‚úÖ Table dim_securite cr√©√©e ou d√©j√† existante.")
            
            # Table dim_sante
            self._execute_sql(cursor, """
            CREATE TABLE IF NOT EXISTS `dim_sante` (
              `id` INT PRIMARY KEY AUTO_INCREMENT,
              `code_dept` VARCHAR(3),
              `annee` INT,
              `esperance_vie` FLOAT
            );
            """, conn, "‚úÖ Table dim_sante cr√©√©e ou d√©j√† existante.")
            
            # Table dim_education
            self._execute_sql(cursor, """
            CREATE TABLE IF NOT EXISTS `dim_education` (
              `id` INT PRIMARY KEY AUTO_INCREMENT,
              `code_departement` VARCHAR(3),
              `annee_fermeture` INT,
              `libelle_departement` VARCHAR(100),
              `nombre_total_etablissements` INT,
              `nb_public` INT,
              `nb_prive` INT,
              `pct_public` FLOAT,
              `pct_prive` FLOAT
            );
            """, conn, "‚úÖ Table dim_education cr√©√©e ou d√©j√† existante.")
            
            # Table dim_environnement
            self._execute_sql(cursor, """
            CREATE TABLE IF NOT EXISTS `dim_environnement` (
              `id` INT PRIMARY KEY AUTO_INCREMENT,
              `code_insee_region` VARCHAR(3),
              `annee` INT,
              `parc_eolien_mw` FLOAT,
              `parc_solaire_mw` FLOAT
            );
            """, conn, "‚úÖ Table dim_environnement cr√©√©e ou d√©j√† existante.")
            
            # Table dim_socio_economie
            self._execute_sql(cursor, """
            CREATE TABLE IF NOT EXISTS `dim_socio_economie` (
              `id` INT PRIMARY KEY AUTO_INCREMENT,
              `annee` INT,
              `pib_euros_par_habitant` FLOAT,
              `code_insee_region` VARCHAR(3),
              `evolution_prix_conso` FLOAT,
              `pib_par_inflation` FLOAT
            );
            """, conn, "‚úÖ Table dim_socio_economie cr√©√©e ou d√©j√† existante.")
            
            # Table dim_technologie
            self._execute_sql(cursor, """
            CREATE TABLE IF NOT EXISTS `dim_technologie` (
              `id` INT PRIMARY KEY AUTO_INCREMENT,
              `annee` INT,
              `depenses_rd_pib` FLOAT
            );
            """, conn, "‚úÖ Table dim_technologie cr√©√©e ou d√©j√† existante.")
            
            # Table dim_demographie
            self._execute_sql(cursor, """
            CREATE TABLE IF NOT EXISTS `dim_demographie` (
              `id` INT PRIMARY KEY AUTO_INCREMENT,
              `annee` INT,
              `code_departement` VARCHAR(3),
              `nom_departement` VARCHAR(100),
              `population_totale` INT,
              `population_hommes` INT,
              `population_femmes` INT,
              `pop_0_19` INT,
              `pop_20_39` INT,
              `pop_40_59` INT,
              `pop_60_74` INT,
              `pop_75_plus` INT
            );
            """, conn, "‚úÖ Table dim_demographie cr√©√©e ou d√©j√† existante.")
            
            # Table fact_resultats_politique
            self._execute_sql(cursor, """
            CREATE TABLE IF NOT EXISTS `fact_resultats_politique` (
              `annee_code_dpt` VARCHAR(10) PRIMARY KEY,
              `id_parti` INT,
              `securite_id` INT,
              `socio_eco_id` INT,
              `sante_id` INT,
              `environnement_id` INT,
              `education_id` INT,
              `demographie_id` INT,
              `technologie_id` INT
            );
            """, conn, "‚úÖ Table fact_resultats_politique cr√©√©e ou d√©j√† existante.")
            
            # Ajout des contraintes de cl√©s √©trang√®res
            self._execute_sql(cursor, """
            ALTER TABLE `fact_resultats_politique` ADD CONSTRAINT `fk_politique` 
            FOREIGN KEY (`id_parti`) REFERENCES `dim_politique` (`id`) ON DELETE SET NULL;
            """, conn, "‚úÖ Contrainte FK ajout√©e: fact_resultats_politique -> dim_politique")
            
            self._execute_sql(cursor, """
            ALTER TABLE `fact_resultats_politique` ADD CONSTRAINT `fk_securite` 
            FOREIGN KEY (`securite_id`) REFERENCES `dim_securite` (`id`) ON DELETE SET NULL;
            """, conn, "‚úÖ Contrainte FK ajout√©e: fact_resultats_politique -> dim_securite")
            
            self._execute_sql(cursor, """
            ALTER TABLE `fact_resultats_politique` ADD CONSTRAINT `fk_socio_eco` 
            FOREIGN KEY (`socio_eco_id`) REFERENCES `dim_socio_economie` (`id`) ON DELETE SET NULL;
            """, conn, "‚úÖ Contrainte FK ajout√©e: fact_resultats_politique -> dim_socio_economie")
            
            self._execute_sql(cursor, """
            ALTER TABLE `fact_resultats_politique` ADD CONSTRAINT `fk_sante` 
            FOREIGN KEY (`sante_id`) REFERENCES `dim_sante` (`id`) ON DELETE SET NULL;
            """, conn, "‚úÖ Contrainte FK ajout√©e: fact_resultats_politique -> dim_sante")
            
            self._execute_sql(cursor, """
            ALTER TABLE `fact_resultats_politique` ADD CONSTRAINT `fk_environnement` 
            FOREIGN KEY (`environnement_id`) REFERENCES `dim_environnement` (`id`) ON DELETE SET NULL;
            """, conn, "‚úÖ Contrainte FK ajout√©e: fact_resultats_politique -> dim_environnement")
            
            self._execute_sql(cursor, """
            ALTER TABLE `fact_resultats_politique` ADD CONSTRAINT `fk_education` 
            FOREIGN KEY (`education_id`) REFERENCES `dim_education` (`id`) ON DELETE SET NULL;
            """, conn, "‚úÖ Contrainte FK ajout√©e: fact_resultats_politique -> dim_education")
            
            self._execute_sql(cursor, """
            ALTER TABLE `fact_resultats_politique` ADD CONSTRAINT `fk_demographie` 
            FOREIGN KEY (`demographie_id`) REFERENCES `dim_demographie` (`id`) ON DELETE SET NULL;
            """, conn, "‚úÖ Contrainte FK ajout√©e: fact_resultats_politique -> dim_demographie")
            
            self._execute_sql(cursor, """
            ALTER TABLE `fact_resultats_politique` ADD CONSTRAINT `fk_technologie` 
            FOREIGN KEY (`technologie_id`) REFERENCES `dim_technologie` (`id`) ON DELETE SET NULL;
            """, conn, "‚úÖ Contrainte FK ajout√©e: fact_resultats_politique -> dim_technologie")
            
            conn.close()
            logger.info("‚úÖ Initialisation de la base de donn√©es termin√©e avec succ√®s!")
            
        except mysql.connector.Error as err:
            logger.error(f"‚ùå Erreur MySQL lors de l'initialisation de la base: {err}")
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©rale lors de l'initialisation de la base: {str(e)}")

    def drop_fact_table(self):
        """
        Supprime la table de faits pour lib√©rer les contraintes avant le rechargement des dimensions
        """
        try:
            conn = mysql.connector.connect(
                host=self._get_host_from_jdbc(),
                database=self.database,
                user=self.user,
                password=self.password
            )
            cursor = conn.cursor()

            # D√©sactiver les v√©rifications de FK
            self._execute_sql(cursor, "SET FOREIGN_KEY_CHECKS=0;", conn, "Foreign key checks disabled.")
            
            # Supprimer la table de faits
            self._execute_sql(cursor, "DROP TABLE IF EXISTS fact_resultats_politique;", conn, 
                            "Table fact_resultats_politique supprim√©e.")
            
            # Recr√©er la structure de la table
            self._execute_sql(cursor, """
            CREATE TABLE IF NOT EXISTS `fact_resultats_politique` (
            `annee_code_dpt` VARCHAR(10) PRIMARY KEY,
            `id_parti` INT,
            `securite_id` INT,
            `socio_eco_id` INT,
            `sante_id` INT,
            `environnement_id` INT,
            `education_id` INT,
            `demographie_id` INT,
            `technologie_id` INT
            );
            """, conn, "Table fact_resultats_politique recr√©√©e.")
            
            # R√©activer les v√©rifications de FK
            self._execute_sql(cursor, "SET FOREIGN_KEY_CHECKS=1;", conn, "Foreign key checks enabled.")
            
            cursor.close()
            conn.close()
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la suppression de la table de faits: {str(e)}")
            return False

    def truncate_and_load_dim(self, table_name, df):
        """
        Vide une table avec TRUNCATE puis y charge des donn√©es
        
        Args:
            table_name: Nom de la table √† vider
            df: DataFrame √† charger
        """
        try:
            if df is None:
                logger.warning(f"‚ö†Ô∏è DataFrame pour {table_name} est None, chargement ignor√©")
                return False
                
            conn = mysql.connector.connect(
                host=self._get_host_from_jdbc(),
                database=self.database,
                user=self.user,
                password=self.password
            )
            cursor = conn.cursor()
            
            # D√©sactiver FK checks
            self._execute_sql(cursor, "SET FOREIGN_KEY_CHECKS=0;", conn, "Foreign key checks disabled.")
            
            # Vider la table avec TRUNCATE
            self._execute_sql(cursor, f"TRUNCATE TABLE {table_name};", conn, f"Table {table_name} vid√©e.")
            
            # Construire l'URL compl√®te avec la base de donn√©es
            full_jdbc_url = f"{self.jdbc_url}/{self.database}"
            
            # Propri√©t√©s de connexion
            props = {
                "user": self.user,
                "password": self.password,
                "driver": self.driver
            }
            
            # Charger les donn√©es en mode append (la table existe d√©j√†)
            df.write.jdbc(
                url=full_jdbc_url,
                table=table_name,
                mode="append",
                properties=props
            )
            
            # R√©activer les FK checks
            self._execute_sql(cursor, "SET FOREIGN_KEY_CHECKS=1;", conn, "Foreign key checks enabled.")
            
            cursor.close()
            conn.close()
            logger.info(f"‚úÖ Donn√©es charg√©es dans {table_name} avec succ√®s")
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement dans {table_name}: {str(e)}")
            return False
    
                
    def generate_consolidated_csv_from_files(
        self,
        election_csv,    # Chemin vers le CSV des donn√©es politiques
        security_csv,    # Chemin vers le CSV de la s√©curit√©
        socio_csv,       # Chemin vers le CSV de la socio-√©conomie
        sante_csv,       # Chemin vers le CSV de la sant√©
        env_csv,         # Chemin vers le CSV de l'environnement
        edu_csv,         # Chemin vers le CSV de l'√©ducation
        demo_csv,        # Chemin vers le CSV de la d√©mographie
        tech_csv,        # Chemin vers le CSV de la technologie
        output_filename="consolidated_data.csv"
    ):
        from pyspark.sql.functions import col, lit, concat, lpad, coalesce, first, trim
        from pyspark.sql.types import StringType

        logger.info("üöÄ G√©n√©ration du fichier consolid√© √† partir des CSV...")

        # Variables constantes pour la lisibilit√©
        DEPT_FILE = "data/politique/departements-france.csv"
        TARGET_YEARS = [2002, 2007, 2012, 2017, 2022]
        
        # Liste des d√©partements d√©sir√©s (m√©tropole, sans Corse)
        # Utilisation d'une compr√©hension de liste plus claire
        desired_depts = [f"{i:02d}" for i in range(1, 96) if i != 20]

        # Fonction d'aide pour lire les CSV avec des options standard
        def read_csv_with_options(path, delimiter=";"):
            return self.spark.read.option("header", "true").option("delimiter", delimiter).csv(path)

        # Fichier mapping d√©partements-r√©gions
        df_depts = self.spark.read.option("header", "true").csv(DEPT_FILE) \
            .select(
                trim(col("code_region")).alias("region"),
                col("code_departement").alias("dept")
            ).filter(col("dept").isin(desired_depts))

        # Lecture et transformation des fichiers d√©partementaux
        # Utilisation d'une structure plus coh√©rente pour les s√©lections
        df_pol = read_csv_with_options(election_csv) \
            .select(
                col("annee").cast("int"),
                lpad(col("code_dept"), 2, "0").alias("dept"),
                col("id_parti").alias("politique (parti)")
            )

        df_sec = read_csv_with_options(security_csv) \
            .select(
                col("Ann√©e").cast("int").alias("annee"),
                lpad(col("D√©partement"), 2, "0").alias("dept"),
                col("D√©lits_total").alias("securite (Nombre_de_d√©lits)")
            )

        df_sat = read_csv_with_options(sante_csv) \
            .select(
                col("Ann√©e").cast("int").alias("annee"),
                lpad(col("CODE_DEP"), 2, "0").alias("dept"),
                col("Esp√©rance_Vie").alias("sante (Esp√©rance_de_Vie_H/F)")
            )

        # Pour l'√©ducation, optimisation de la conversion et du format
        df_ed = read_csv_with_options(edu_csv) \
            .select(
                col("annee_fermeture").cast("int").alias("annee"),
                lpad(col("code_departement").cast("int").cast("string"), 2, "0").alias("dept"),
                col("nombre_total_etablissements").cast("int").alias("education (Nombre_Total_√âtablissements)")
            ) \
            .filter(col("annee").isin(TARGET_YEARS)) \
            .groupBy("annee", "dept") \
            .agg(first("education (Nombre_Total_√âtablissements)").alias("education (Nombre_Total_√âtablissements)"))

        df_dem = read_csv_with_options(demo_csv) \
            .select(
                col("Ann√©e").cast("int").alias("annee"),
                lpad(col("Code_D√©partement"), 2, "0").alias("dept"),
                col("E_Total").alias("demographie (Population_Totale)")
            )

        df_tech = read_csv_with_options(tech_csv) \
            .select(
                col("annee").cast("int"),
                col("dird_pib_france_pourcentages").alias("technologie (D√©penses_en_R&D_en_pourcentages)")
            )

        # Factorisation du code pour les fichiers r√©gionaux
        def process_regional_file(csv_path, value_col_name, output_col_name):
            return read_csv_with_options(csv_path) \
                .select(
                    coalesce(col("Ann√©e"), col("ann√©e")).cast("int").alias("annee"),
                    trim(col("Code_INSEE_R√©gion")).alias("region"),
                    col(value_col_name).alias(output_col_name)
                ) \
                .groupBy("annee", "region") \
                .agg(first(output_col_name).alias(output_col_name)) \
                .join(df_depts, on="region", how="inner") \
                .drop("region")

        # Application de la fonction factoris√©E
        df_soc = process_regional_file(socio_csv, "PIB_par_inflation", "socio_economie (PIB_par_Inflation)")
        df_envr = process_regional_file(env_csv, "Parc_install√©_√©olien_MW", "environnemental (Parc_install√©_√©olien_MW)")

        # Jointure progressive et lisible avec des variables interm√©diaires pour faciliter le d√©bogage
        df_base = df_pol.join(df_sec, ["annee", "dept"], "full_outer")
        df_mid = df_base.join(df_soc, ["annee", "dept"], "full_outer") \
                        .join(df_sat, ["annee", "dept"], "full_outer") \
                        .join(df_envr, ["annee", "dept"], "full_outer")
        df_join = df_mid.join(df_ed, ["annee", "dept"], "left") \
                        .join(df_dem, ["annee", "dept"], "full_outer") \
                        .join(df_tech, ["annee"], "left")

        # Filtrage avec une condition lisible
        df_filtered = df_join.filter(
            (col("annee").isin(TARGET_YEARS)) & (col("dept").isin(desired_depts))
        )

        # Ajout d'une colonne cl√© pour identification
        df_with_key = df_filtered.withColumn(
            "annee_code_dpt", 
            concat(col("annee").cast("string"), lit("_"), col("dept"))
        )

        # Colonnes finales pour le rapport
        output_columns = [
            "annee_code_dpt",
            "politique (parti)",
            "securite (Nombre_de_d√©lits)",
            "socio_economie (PIB_par_Inflation)",
            "sante (Esp√©rance_de_Vie_H/F)",
            "environnemental (Parc_install√©_√©olien_MW)",
            "education (Nombre_Total_√âtablissements)",
            "demographie (Population_Totale)",
            "technologie (D√©penses_en_R&D_en_pourcentages)"
        ]
        
        # S√©lection et ordonnancement
        df_final = df_with_key.select(*output_columns).orderBy("annee_code_dpt")

        logger.info("‚úÖ Donn√©es consolid√©es pr√™tes. Aper√ßu :")
        df_final.show(10, truncate=False)

        self.save_to_csv(df_final, output_filename)