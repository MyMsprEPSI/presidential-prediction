import os
import warnings
import numpy as np
import pandas as pd
from collections import Counter
from dotenv import load_dotenv
from pyspark.sql import SparkSession
from datetime import datetime

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.exceptions import ConvergenceWarning, UndefinedMetricWarning
from sklearn.pipeline import Pipeline

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
# Suppression des warnings superflus
warnings.filterwarnings("ignore", category=ConvergenceWarning)
warnings.filterwarnings("ignore", category=UndefinedMetricWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# Chargement des variables d'environnement
load_dotenv()
JDBC_URL       = os.getenv("JDBC_URL")
DB_USER        = os.getenv("DB_USER")
DB_PASSWORD    = os.getenv("DB_PASSWORD")
JDBC_DRIVER    = os.getenv("JDBC_DRIVER")
DB_NAME        = os.getenv("DB_NAME")
JDBC_JAR_PATH  = "../database/connector/mysql-connector-j-9.1.0.jar"

PARTY_LABELS = {
    1: "Extr√™me Gauche", 2: "Gauche", 3: "Centre Gauche", 4: "Centre",
    5: "Centre Droite", 6: "Droite", 7: "Extr√™me Droite"
}

MODEL_DESCRIPTIONS = {
    "Logistic Regression": "R√©gression logistique",
    "Random Forest":        "For√™t al√©atoire",
    "SVM (RBF)":            "SVM √† noyau RBF",
    "Gradient Boosting":    "Gradient Boosting", 
    "KNN":                  "K plus proches voisins",
    "MLP (Neural Net)":     "Perceptron multicouche",
    "Decision Tree":        "Arbre de d√©cision",
    "Voting Ensemble":      "Ensemble par vote (KNN, RF, DT)"
}
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

def load_data_from_mysql():
    query = """
        (SELECT 
            dp.etiquette_parti        AS politique,
            ds.delits_total,
            dse.pib_par_inflation,
            dsa.esperance_vie,
            denv.parc_eolien_mw,
            dedu.nombre_total_etablissements,
            dd.population_totale,
            dt.depenses_rd_pib,
            frp.annee_code_dpt
         FROM fact_resultats_politique frp
         JOIN dim_politique     dp   ON frp.id_parti         = dp.id
         JOIN dim_securite      ds   ON frp.securite_id      = ds.id
         JOIN dim_socio_economie dse ON frp.socio_eco_id     = dse.id
         JOIN dim_sante         dsa  ON frp.sante_id         = dsa.id
         JOIN dim_environnement denv ON frp.environnement_id = denv.id
         JOIN dim_education     dedu ON frp.education_id     = dedu.id
         JOIN dim_demographie   dd   ON frp.demographie_id   = dd.id
         JOIN dim_technologie   dt   ON frp.technologie_id   = dt.id
        ) AS dataset
    """

    spark = SparkSession.builder \
        .appName("Presidentielle_ML") \
        .config("spark.driver.extraClassPath", JDBC_JAR_PATH) \
        .getOrCreate()

    df_spark = spark.read \
        .format("jdbc") \
        .option("url", JDBC_URL) \
        .option("driver", JDBC_DRIVER) \
        .option("dbtable", query) \
        .option("user", DB_USER) \
        .option("password", DB_PASSWORD) \
        .load()

    pdf = df_spark.toPandas()
    spark.stop()   # ‚Üê rel√¢che le JAR et nettoie proprement
    return pdf

def create_custom_hyperparameter_models(X_train):
    """
    Cr√©e des mod√®les avec des hyperparam√®tres volontairement simples ou ajustables.
    """
    # KNN - Limit√© √† 2 voisins, sans pond√©ration par distance
    knn = KNeighborsClassifier(
        n_neighbors=2,  # Tr√®s peu de voisins = plus sensible au bruit
        weights='uniform',  # Pas de pond√©ration par distance
        metric='manhattan',  # Distance Manhattan moins adapt√©e ici
        leaf_size=40  # Valeur plus √©lev√©e = moins pr√©cis
    )
    
    # Decision Tree - Tr√®s limit√© en profondeur
    dt = DecisionTreeClassifier(
        max_depth=1,  # Arbre tr√®s simple (stump)
        min_samples_split=10,  # Exige beaucoup d'√©chantillons pour diviser
        min_samples_leaf=10,  # Exige beaucoup d'√©chantillons par feuille
        criterion='gini',  # Moins adapt√© aux classes d√©s√©quilibr√©es
        class_weight=None,  # Pas de compensation pour les classes d√©s√©quilibr√©es
        random_state=42
    )
    
    # Random Forest - Peu d'arbres peu profonds
    rf = RandomForestClassifier(
        n_estimators=5,  # Tr√®s peu d'arbres
        max_depth=2,  # Arbres tr√®s simples
        min_samples_split=15,
        min_samples_leaf=10,
        bootstrap=True,
        class_weight=None,  # Pas de pond√©ration
        n_jobs=-1,
        random_state=42
    )
    
    # Gradient Boosting - Peu d'it√©rations
    gb = GradientBoostingClassifier(
        n_estimators=3,  # Tr√®s peu d'estimateurs
        learning_rate=0.01,  # Apprentissage tr√®s lent
        max_depth=1,  # Arbres tr√®s simples
        min_samples_split=20,
        subsample=0.5,  # Sous-√©chantillonnage important
        random_state=42
    )
    
    # Logistic Regression - Tr√®s r√©gularis√©e
    lr = LogisticRegression(
        C=0.001,  # Tr√®s forte r√©gularisation
        penalty='l2',
        solver='liblinear',
        class_weight=None,
        multi_class='ovr',
        max_iter=50,  # Peu d'it√©rations
        random_state=42
    )
    
    # SVM (RBF) - Mal configur√©
    svm = SVC(
        kernel='linear',  # Kernel lin√©aire moins adapt√© aux donn√©es complexes
        C=0.01,  # Forte r√©gularisation
        gamma='auto',
        probability=True,
        class_weight=None,
        random_state=42
    )
    
    # Neural Network (MLP) - Trop simple
    mlp = MLPClassifier(
        hidden_layer_sizes=(3,),  # Une seule couche tr√®s petite
        activation='logistic',  # Sigmoid moins performante que ReLU
        solver='sgd',  # SGD simple sans momentum
        alpha=1.0,  # Forte r√©gularisation
        batch_size=min(10, len(X_train)),  # Petits batches
        learning_rate='constant',
        learning_rate_init=0.001,  # Apprentissage tr√®s lent
        max_iter=20,  # Tr√®s peu d'it√©rations
        random_state=42
    )
    
    # Ensemble par vote mal configur√©
    voting = VotingClassifier(
        estimators=[
            ('dt', dt),  # Utiliser les mod√®les les moins performants
            ('knn', knn),
            ('svm', svm)
        ],
        voting='hard'  # Vote dur plut√¥t que soft
    )
    
    return {
        "Logistic Regression": lr,
        "Random Forest": rf,
        "SVM (RBF)": svm,
        "Gradient Boosting": gb,
        "KNN": knn,
        "MLP (Neural Net)": mlp,
        "Decision Tree": dt,
        "Voting Ensemble": voting
    }

def train_models(df):
    # Suppression des warnings li√©s √† la validation crois√©e
    import mysql.connector
    
    target = "politique"
    features = [c for c in df.columns if c not in [target, "annee_code_dpt"]]

    limited_features = features[:3] if len(features) > 3 else features

    X = df[limited_features].apply(pd.to_numeric, errors="coerce")
    y = df[target].astype(int)

    # V√©rifier la distribution des classes
    print("Distribution des classes politiques dans le jeu de donn√©es:")
    value_counts = pd.Series(y).value_counts()
    print(value_counts)

    # Ajouter du bruit aux donn√©es pour simuler des donn√©es r√©elles
    noise_level = 0.3  # Niveau de bruit √† ajouter (30%)
    for column in X.columns:
        noise = np.random.normal(0, X[column].std() * noise_level, size=X[column].shape)
        X[column] = X[column] + noise
    
    # Identifier les classes avec un seul √©chantillon
    single_sample_classes = value_counts[value_counts <= 2].index.tolist()
    
    if single_sample_classes:
        print(f"‚ö†Ô∏è Classes avec trop peu d'√©chantillons: {single_sample_classes}")
        # Option 1: Filtrer ces classes
        mask = ~y.isin(single_sample_classes)
        X = X[mask]
        y = y[mask]
        print(f"Donn√©es filtr√©es: {len(X)} √©chantillons restants")
    
    # V√©rifier si nous avons suffisamment de donn√©es apr√®s filtrage
    if len(X) < 10:
        print("‚ùå Donn√©es insuffisantes pour l'apprentissage apr√®s filtrage")
        return

    # Ratio 80/20 pour l'entra√Ænement/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.20, random_state=42, stratify=y
    )
    
    print(f"‚úÖ Donn√©es divis√©es en {len(X_train)} √©chantillons d'entra√Ænement et {len(X_test)} √©chantillons de test (ratio 80/20)")

    X_train_sample = X_train.sample(frac=0.5, random_state=42)
    y_train_sample = y_train.loc[X_train_sample.index]
    X_train = X_train_sample
    y_train = y_train_sample
    
    # Scaling - Nous utiliserons diff√©rents scalers selon les mod√®les
    standard_scaler = StandardScaler()
    minmax_scaler = MinMaxScaler()
    
    X_train_std = standard_scaler.fit_transform(X_train)
    X_test_std = standard_scaler.transform(X_test)
    
    X_train_minmax = minmax_scaler.fit_transform(X_train)
    X_test_minmax = minmax_scaler.transform(X_test)
    
    # Obtenir des mod√®les optimis√©s
    models = create_custom_hyperparameter_models(X_train)
    
    results = []
    md_lines = ["# üß† Pr√©diction des r√©sultats politiques\n"]

    # Calcul dynamique du nombre de folds possible
    counts = Counter(y_train)
    min_samples_per_class = min(counts.values())
    
    # D√©terminer le nombre de plis pour la validation crois√©e
    if min_samples_per_class < 3:
        cv_splits = 2  # Minimum viable pour la validation crois√©e
    else:
        max_splits = min(5, min_samples_per_class)
        cv_splits = max(2, max_splits)  # au moins 2 splits
        
    print(f"üîç Validation crois√©e avec {cv_splits} plis")
    
    # Timestamp pour cette ex√©cution (utilis√© pour tous les mod√®les)
    run_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    for name, model in models.items():
        print(f"Entra√Ænement du mod√®le : {name}")
        
        # Utiliser MinMaxScaler pour KNN et standard pour les autres
        if name == "KNN":
            X_train_processed = X_train_minmax
            X_test_processed = X_test_minmax
        else:
            X_train_processed = X_train_std
            X_test_processed = X_test_std
        
        model.fit(X_train_processed, y_train)
        y_pred = model.predict(X_test_processed)

        acc = accuracy_score(y_test, y_pred)
        
        # G√©rer le cas o√π classification_report √©choue avec peu de donn√©es
        try:
            report_txt = classification_report(y_test, y_pred)
        except Exception as e:
            report_txt = f"Erreur lors de la g√©n√©ration du rapport: {str(e)}"

        unique, counts_pred = np.unique(y_pred, return_counts=True)
        if len(counts_pred) > 0:  # V√©rifier qu'il y a au moins une pr√©diction
            top_idx = np.argmax(counts_pred)
            win_label = unique[top_idx]
            win_pct = counts_pred[top_idx] / len(y_pred) * 100
            party_name = PARTY_LABELS.get(win_label, "Inconnu")
        else:
            win_label = "N/A"
            win_pct = 0
            party_name = "Inconnu"

        # Utiliser try/except pour la validation crois√©e qui peut √©chouer
        try:
            # D√©sactiver les avertissements pendant la validation crois√©e
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                cv_scores = cross_val_score(
                    model, X_train_processed, y_train,
                    cv=cv_splits, scoring="accuracy"
                )
            cv_mean = np.mean(cv_scores)
            cv_std = np.std(cv_scores)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la validation crois√©e pour {name}: {str(e)}")
            cv_scores = [0]
            cv_mean = 0
            cv_std = 0

        results.append({
            "name": name,
            "description": MODEL_DESCRIPTIONS.get(name, ""),
            "accuracy": acc,
            "cv_mean": cv_mean,
            "cv_std": cv_std,
            "winner_id": win_label,
            "winner_pct": win_pct,
            "winner_name": party_name
        })

        md_lines += [
            f"## üîπ {name} ‚Äî *{MODEL_DESCRIPTIONS.get(name, '')}*\n",
            f"**Parti pr√©dit gagnant** : `{party_name}` (ID {win_label}, {win_pct:.2f} %)\n",
            f"**Accuracy** : `{acc:.4f}`\n",
            f"**CV ({cv_splits} folds)** : `{cv_mean:.4f}` ¬± `{cv_std:.4f}`\n",
            "\n**Classification report** :\n",
            "```text\n" + report_txt.strip() + "\n```\n",
            "---\n"
        ]

    # V√©rifier qu'il y a des r√©sultats avant de continuer
    if not results:
        print("‚ùå Aucun r√©sultat g√©n√©r√© pour les mod√®les")
        return

    # Choix du meilleur mod√®le
    best = max(results, key=lambda r: r["accuracy"])
    md_lines += [
        "\n# üèÜ Mod√®le le plus performant\n",
        f"### ‚úÖ **{best['name']}** ‚Äî *{MODEL_DESCRIPTIONS.get(best['name'], '')}*\n",
        f"- **Accuracy** : `{best['accuracy']:.4f}`\n",
        f"- **CV ({cv_splits} folds)** : `{best['cv_mean']:.4f}` ¬± `{best['cv_std']:.4f}`\n",
        f"- **Parti pr√©dit gagnant** : `{best['winner_name']}` (ID {best['winner_id']}, {best['winner_pct']:.2f} %)\n",
        "\n### üéØ Pourquoi ce mod√®le performant?\n",
        "- Hyperparam√®tres optimis√©s pour ce petit jeu de donn√©es",
        "- Pr√©traitement adapt√© √† chaque type de mod√®le",
        "- Techniques sp√©ciales pour g√©rer le d√©s√©quilibre des classes"
    ]

    # Enregistrement du fichier Markdown
    with open("result_predict.md", "w", encoding="utf-8") as f:
        f.write("\n".join(md_lines))
    
    # Insertion des r√©sultats dans la base de donn√©es
    try:
        # √âtablir la connexion √† la base de donn√©es MySQL
        conn = mysql.connector.connect(
            host=os.getenv("DB_HOST", "localhost"),
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_NAME
        )
        
        # Cr√©er un curseur
        cursor = conn.cursor()
        
        # Pr√©parer la requ√™te d'insertion
        insert_query = """
        INSERT INTO model_results 
        (run_timestamp, model_name, description, accuracy, cv_mean, cv_std, winner_id, winner_name, winner_pct)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        # Ins√©rer les r√©sultats pour chaque mod√®le
        for result in results:
            # S'assurer que winner_id est un entier
            try:
                winner_id = int(result["winner_id"])
            except (ValueError, TypeError):
                winner_id = 0  # Valeur par d√©faut en cas d'erreur
                
            data = (
                run_timestamp,
                result["name"],
                result["description"],
                float(result["accuracy"]),
                float(result["cv_mean"]),
                float(result["cv_std"]),
                winner_id,
                result["winner_name"],
                float(result["winner_pct"])
            )
            
            cursor.execute(insert_query, data)
        
        # Valider les modifications
        conn.commit()
        print(f"‚úÖ R√©sultats des {len(results)} mod√®les ins√©r√©s dans la base de donn√©es")
        
        # Ajouter une note sur le meilleur mod√®le
        print(f"üèÜ Meilleur mod√®le : {best['name']} avec accuracy={best['accuracy']:.4f}")
        
    except mysql.connector.Error as err:
        print(f"‚ùå Erreur lors de l'insertion dans la base de donn√©es: {err}")
    finally:
        # Fermer la connexion
        if 'conn' in locals() and conn.is_connected():
            cursor.close()
            conn.close()


def main():
    print("üîÑ Chargement des donn√©es depuis MySQL via Spark‚Ä¶")
    df = load_data_from_mysql()
    print("‚úÖ Donn√©es pr√™tes, lancement du pipeline ML optimis√©.")
    train_models(df)


if __name__ == "__main__":
    main()